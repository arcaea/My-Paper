{"cells":[{"cell_type":"markdown","metadata":{},"source":["# import"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["!export LC_ALL=\"en_US.UTF-8\"\n","!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n","!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n","!ldconfig /usr/lib64-nvidia\n","\n","import os\n","import json\n","import time\n","import torch\n","import random\n","import warnings\n","import evaluate\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","from torchcrf import CRF\n","import torch.optim as optim\n","from sklearn import metrics\n","from torch.utils import data\n","from datasets import load_dataset\n","from collections import namedtuple\n","from dataclasses import dataclass, field, asdict\n","from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n","from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n","from torch.utils.data import Dataset # from datasets import Dataset\n","from transformers import Trainer, BertConfig, AutoTokenizer, TrainingArguments, AdamW, get_linear_schedule_with_warmup\n","\n","\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0' # cuda\n","# os.environ['CUDA_VISIBLE_DEVICES'] = '1' # cpu\n","\n","\n","\n","from IPython.display import clear_output\n","clear_output()"]},{"cell_type":"markdown","metadata":{},"source":["# Var"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["batch_size = 4\n","MAX_LEN = 256 - 2\n","train_path = 'data/train.txt'\n","bert_model = 'bert-base-chinese'\n","tokenizer = AutoTokenizer.from_pretrained(bert_model)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["VOCAB = ('<PAD>', '[CLS]', '[SEP]', 'O', 'B-BODY', 'I-BODY',\n","         'B-SYMP', 'I-SYMP', 'B-INST', 'I-INST', 'B-EXAM', 'I-EXAM',\n","         'B-CHEM', 'I-CHEM','B-DISE', 'I-DISE', 'B-DRUG', 'I-DRUG',\n","         'B-SUPP', 'I-SUPP', 'B-TREAT', 'I-TREAT', 'B-TIME', 'I-TIME')\n","\n","tag2idx = {tag: idx for idx, tag in enumerate(VOCAB)}\n","idx2tag = {idx: tag for idx, tag in enumerate(VOCAB)}"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["labels = ['O','B-BODY','I-BODY', 'B-SYMP', 'I-SYMP', 'B-INST', 'I-INST', 'B-EXAM', 'I-EXAM','B-CHEM', 'I-CHEM',\n","          'B-DISE', 'I-DISE', 'B-DRUG', 'I-DRUG', 'B-SUPP', 'I-SUPP', 'B-TREAT', 'I-TREAT', 'B-TIME', 'I-TIME']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(labels)"]},{"cell_type":"markdown","metadata":{"id":"r9BqpcTuAsc5"},"source":["# Dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"kGTNz-EvSujr"},"outputs":[],"source":["class NerDataset(Dataset):\n","    ''' Generate our dataset '''\n","\n","    def __init__(self, f_path):\n","        self.sents = []\n","        self.tags_li = []\n","\n","        with open(f_path, 'r', encoding='utf-8') as f:\n","            lines = [line.split('\\n')[0] for line in f.readlines() if len(line.strip())!=0]\n","\n","        tags =  [line.split('\\t')[1] for line in lines]\n","        words = [line.split('\\t')[0] for line in lines]\n","\n","        word, tag = [], []\n","        for char, t in zip(words, tags):\n","            if char != '。':\n","                word.append(char)\n","                tag.append(t)\n","            else:\n","                if len(word) > MAX_LEN:\n","                  self.sents.append(['[CLS]'] + word[:MAX_LEN] + ['[SEP]'])\n","                  self.tags_li.append(['[CLS]'] + tag[:MAX_LEN] + ['[SEP]'])\n","                else:\n","                  self.sents.append(['[CLS]'] + word + ['[SEP]'])\n","                  self.tags_li.append(['[CLS]'] + tag + ['[SEP]'])\n","                word, tag = [], []\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx]\n","        token_ids = tokenizer.convert_tokens_to_ids(words)\n","        laebl_ids = [tag2idx[tag] for tag in tags]\n","        seqlen = len(laebl_ids)\n","        return token_ids, laebl_ids, seqlen\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","def PadBatch(batch):#[Pad]\n","    maxlen = max([i[2] for i in batch])\n","    token_tensors = torch.LongTensor([i[0] + [0] * (maxlen - len(i[0])) for i in batch])\n","    label_tensors = torch.LongTensor([i[1] + [0] * (maxlen - len(i[1])) for i in batch])\n","    # mask = (token_tensors > 0)\n","    mask = (token_tensors > 0).to(torch.bool)\n","    return token_tensors, label_tensors, mask"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Mamba 的 config 類引用了這個詞: https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/config_mamba.py\n","@dataclass\n","class MambaConfig:\n","    d_model: int = 2560\n","    n_layer: int = 64\n","    vocab_size: int = 50277 # 50277\n","    ssm_cfg: dict = field(default_factory=dict)\n","    rms_norm: bool = True\n","    residual_in_fp32: bool = True\n","    fused_add_norm: bool = True\n","    # pad_vocab_size_multiple: int = 8\n","    pad_vocab_size_multiple: int = 16\n","    tie_embeddings = True\n","\n","    def to_json_string(self):\n","        return json.dumps(asdict(self))\n","\n","    def to_dict(self):\n","        return asdict(self)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# 用於分類的頭部類別的定義\n","class MambaClassificationHead(nn.Module):\n","    def __init__(self, d_model, num_classes, **kwargs):\n","        super(MambaClassificationHead, self).__init__()\n","        # 使用線性圖層根據輸入執行分類，該輸入的大小d_model且num_classes需要排序。\n","        self.classification_head = nn.Linear(d_model, num_classes, **kwargs)\n","\n","    def forward(self, hidden_states):\n","        return self.classification_head(hidden_states)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class MambaTextClassification(MambaLMHeadModel):\n","    def __init__(\n","        self,\n","        config: MambaConfig,\n","        initializer_cfg=None,\n","        device=None,\n","        dtype=None,\n","    ) -> None:\n","        super().__init__(config, initializer_cfg, device, dtype)\n","\n","        # 使用 MambaClassificationHead 創建一個分類器，輸入大小為 d_model，類號為 len(labels)。\n","        # self.classification_head = nn.Linear(config.d_model, len(labels))\n","        # self.crf = CRF(len(labels), batch_first=True)\n","\n","        self.classification_head = MambaClassificationHead(d_model=config.d_model, num_classes=len(tag2idx))\n","        self.crf = CRF(len(tag2idx), batch_first=True)  \n","                            \n","        del self.lm_head\n","\n","    def forward(self, input_ids, tags, mask, is_test=False):\n","        # 通過原生模型發送input_ids以接收hidden_states。\n","        emissions = self.backbone(input_ids)\n","\n","        # # 取二維emissions的平均值，創建具有代表性的 [CLS] 特徵\n","        # mean_emissions = emissions.mean(dim=1)\n","\n","        # 將mean_emissions通過分類器的頂部來接收logits_emissions。\n","        logits_emissions = self.classification_head(emissions)\n","\n","        if not is_test: # Training，return loss\n","            loss=-self.crf.forward(logits_emissions, tags, mask, reduction='mean')\n","            return loss\n","        else: # Testing，return decoding\n","            decode=self.crf.decode(logits_emissions, mask)\n","            return decode\n","\n","    def predict(self, text, tokenizer, id2label=None):\n","        input_ids = torch.tensor(tokenizer(text)['input_ids'], device=device)[None] # device = 'cuda'\n","        with torch.no_grad():\n","          logits = self.forward(input_ids).logits[0]\n","          label = np.argmax(logits.cpu().numpy())\n","\n","        if id2label is not None:\n","          return id2label[label]\n","        else:\n","          return label\n","\n","    @classmethod\n","    def from_pretrained(cls, pretrained_model_name, device=None, dtype=None, **kwargs):\n","        # 從之前訓練的模型載入配置。\n","        config_data = load_config_hf(pretrained_model_name)\n","        config = MambaConfig(**config_data)\n","\n","        # 從配置中初始化模型，並將其傳輸到所需的設備和數據類型。\n","        model = cls(config, device=device, dtype=dtype, **kwargs)\n","\n","        # 載入以前訓練的模型狀態。\n","        model_state_dict = load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype)\n","        model.load_state_dict(model_state_dict, strict=False)\n","\n","        # 列印出新初始化的嵌入參數。\n","        print(\"Newly initialized embedding:\", set(model.state_dict().keys()) - set(model_state_dict.keys()))\n","        return model"]},{"cell_type":"markdown","metadata":{"id":"dYMEg1XAAex5"},"source":["# Main.ipynb"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"ufTXiTWhR8ra"},"outputs":[],"source":["def train(e, model, iterator, optimizer, scheduler, device):\n","    start_time = time.time()  # Record the start time\n","\n","    model.train().to(device)\n","    losses = 0.0\n","    step = 0\n","    for i, batch in enumerate(iterator):\n","        step += 1\n","        x, y, z = batch\n","        x = x.to(device)\n","        y = y.to(device)\n","        z = z.to(device)\n","\n","        loss = model(x, y, z)\n","        losses += loss.item()\n","        \"\"\" Gradient Accumulation \"\"\"\n","        '''\n","          full_loss = loss / 2                            # normalize loss\n","          full_loss.backward()                            # backward and accumulate gradient\n","          if step % 2 == 0:\n","              optimizer.step()                            # update optimizer\n","              scheduler.step()                            # update scheduler\n","              optimizer.zero_grad()                       # clear gradient\n","        '''\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","    end_time = time.time()  # Record the end time\n","    epoch_time = end_time - start_time\n","\n","    print(\"Epoch: {}, Loss:{:.4f}, epoch_time:{:.2f} sec\".format(e, losses/step, epoch_time))\n","\n","def validate(e, model, iterator, device):\n","    start_time = time.time()  # Record the start time\n","\n","    model.eval()\n","    Y, Y_hat = [], []\n","    losses = 0\n","    step = 0\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            step += 1\n","\n","            x, y, z = batch\n","            x = x.to(device)\n","            y = y.to(device)\n","            z = z.to(device)\n","\n","            y_hat = model(x, y, z, is_test=True)\n","\n","            loss = model(x, y, z)\n","            losses += loss.item()\n","            # Save prediction\n","            for j in y_hat:\n","              Y_hat.extend(j)\n","            # Save labels\n","            mask = (z==1)\n","            y_orig = torch.masked_select(y, mask)\n","            Y.append(y_orig.cpu())\n","\n","    Y = torch.cat(Y, dim=0).numpy()\n","    Y_hat = np.array(Y_hat)\n","    acc = (Y_hat == Y).mean()*100\n","\n","    end_time = time.time()  # Record the end time\n","    epoch_time = end_time - start_time\n","\n","    print(\"Epoch: {}, Val Loss:{:.4f}, Val Acc:{:.3f}, epoch_time:{:.2f} sec\".format(e, losses/step, acc, epoch_time))\n","    return model, losses/step, acc\n","\n","def test(model, iterator, device):\n","    model.eval()\n","    Y, Y_hat = [], []\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            x, y, z = batch\n","            x = x.to(device)\n","            z = z.to(device)\n","            y_hat = model(x, y, z, is_test=True)\n","            # Save prediction\n","            for j in y_hat:\n","              Y_hat.extend(j)\n","            # Save labels\n","            mask = (z==1).cpu()\n","            y_orig = torch.masked_select(y, mask)\n","            Y.append(y_orig)\n","\n","    Y = torch.cat(Y, dim=0).numpy()\n","    y_true = [idx2tag[i] for i in Y]\n","    y_pred = [idx2tag[i] for i in Y_hat]\n","\n","    return y_true, y_pred\n","\n","def main(batch_size=64, lr=0.001, n_epochs=10, trainset=\"data/train.txt\", validset=\"data/msra_eval.txt\", testset=\"data/test.txt\"):\n","\n","    best_model = None\n","    _best_val_loss = 1e18\n","    _best_val_acc = 1e-18\n","\n","    model = MambaTextClassification.from_pretrained(\"state-spaces/mamba-130m\").to(device)\n","    \n","    print('Initial model Done.')\n","    train_dataset = NerDataset(trainset)\n","    eval_dataset = NerDataset(validset)\n","    test_dataset = NerDataset(testset)\n","    print('Load Data Done.')\n","\n","    train_iter = data.DataLoader(dataset=train_dataset,\n","                                 batch_size=batch_size,\n","                                 shuffle=True,\n","                                 num_workers=4,\n","                                 collate_fn=PadBatch)\n","\n","    eval_iter = data.DataLoader(dataset=eval_dataset,\n","                                 batch_size=batch_size,\n","                                 shuffle=False,\n","                                 num_workers=4,\n","                                 collate_fn=PadBatch)\n","\n","    test_iter = data.DataLoader(dataset=test_dataset,\n","                                batch_size=batch_size,\n","                                shuffle=False,\n","                                num_workers=4,\n","                                collate_fn=PadBatch)\n","\n","    #optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=0.01)\n","    optimizer = AdamW(model.parameters(), lr=lr, eps=1e-6)\n","\n","    # Warmup\n","    len_dataset = len(train_dataset)\n","    epoch = n_epochs\n","    batch_size = batch_size\n","    total_steps = (len_dataset // batch_size) * epoch if len_dataset % batch_size == 0 else (len_dataset // batch_size + 1) * epoch\n","\n","    warm_up_ratio = 0.1 # Define 10% steps\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warm_up_ratio * total_steps, num_training_steps = total_steps)\n","\n","    print('Start Train...,')\n","    for epoch in range(1, n_epochs+1):\n","\n","        train(epoch, model, train_iter, optimizer, scheduler, device)\n","        candidate_model, loss, acc = validate(epoch, model, eval_iter, device)\n","\n","        if loss < _best_val_loss and acc > _best_val_acc:\n","          best_model = candidate_model\n","          _best_val_loss = loss\n","          _best_val_acc = acc\n","\n","        print(\"=============================================\")\n","\n","    y_test, y_pred = test(best_model, test_iter, device)\n","    print(metrics.classification_report(y_test, y_pred, labels=labels, digits=3))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["main(n_epochs=5)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
