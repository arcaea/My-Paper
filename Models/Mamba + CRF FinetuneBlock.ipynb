{"cells":[{"cell_type":"markdown","metadata":{},"source":["# import"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["!export LC_ALL=\"en_US.UTF-8\"\n","!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n","!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n","!ldconfig /usr/lib64-nvidia\n","\n","import os\n","import time\n","import random\n","import warnings\n","import evaluate\n","import numpy as np\n","import pandas as pd\n","from torchcrf import CRF\n","import torch.optim as optim\n","from sklearn import metrics\n","from torch.utils import data\n","from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n","from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n","from torch.utils.data import Dataset # from datasets import Dataset\n","from transformers import Trainer, BertConfig, AutoTokenizer, TrainingArguments, AdamW, get_linear_schedule_with_warmup\n","\n","from __future__ import annotations\n","import math\n","import json\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from dataclasses import dataclass\n","from einops import rearrange, repeat, einsum\n","\n","\n","# import urllib.request\n","# from tqdm import tqdm\n","# from zipfile import ZipFile\n","# from torch.nn import functional as F\n","\n","# torch.autograd.set_detect_anomaly(True)\n","\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0' # cuda\n","# os.environ['CUDA_VISIBLE_DEVICES'] = '1' # cpu\n","\n","\n","\n","from IPython.display import clear_output\n","clear_output()"]},{"cell_type":"markdown","metadata":{},"source":["# Var"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","# device = 'cpu'\n","device"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Configuration flags and hyperparameters\n","USE_MAMBA = True # USE_MAMBA = 1\n","DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM = False # DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM = 0"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["batch_size = 4\n","MAX_LEN = 256 - 2\n","train_path = 'data/train.txt'\n","test_path = 'data/test.txt'\n","valid_path = 'data/msra_eval.txt'\n","bert_model = 'bert-base-chinese'\n","tokenizer = AutoTokenizer.from_pretrained(bert_model)\n","VOCAB_size = len(tokenizer)\n","\n","# args\n","d_model = 128\n","state_size = 128  # Example state size\n","seq_len = MAX_LEN  # Example sequence length\n","last_batch_size = batch_size  # only for the very last batch of the dataset\n","current_batch_size = batch_size\n","different_batch_size = False\n","h_new = None\n","temp_buffer = None"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["VOCABofTag = ('<PAD>', '[CLS]', '[SEP]', 'O', 'B-BODY', 'I-BODY',\n","              'B-SYMP', 'I-SYMP', 'B-INST', 'I-INST', 'B-EXAM', 'I-EXAM',\n","              'B-CHEM', 'I-CHEM','B-DISE', 'I-DISE', 'B-DRUG', 'I-DRUG',\n","              'B-SUPP', 'I-SUPP', 'B-TREAT', 'I-TREAT', 'B-TIME', 'I-TIME')\n","\n","tag2idx = {tag: idx for idx, tag in enumerate(VOCABofTag)}\n","idx2tag = {idx: tag for idx, tag in enumerate(VOCABofTag)}"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["labels = ['O','B-BODY','I-BODY', 'B-SYMP', 'I-SYMP', 'B-INST', 'I-INST', 'B-EXAM', 'I-EXAM','B-CHEM', 'I-CHEM',\n","          'B-DISE', 'I-DISE', 'B-DRUG', 'I-DRUG', 'B-SUPP', 'I-SUPP', 'B-TREAT', 'I-TREAT', 'B-TIME', 'I-TIME']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(VOCABofTag)"]},{"cell_type":"markdown","metadata":{"id":"r9BqpcTuAsc5"},"source":["# Dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"kGTNz-EvSujr"},"outputs":[],"source":["class NerDataset(Dataset):\n","    ''' Generate our dataset '''\n","\n","    def __init__(self, f_path):\n","        self.sents = []\n","        self.tags_li = []\n","\n","        with open(f_path, 'r', encoding='utf-8') as f:\n","            lines = [line.split('\\n')[0] for line in f.readlines() if len(line.strip())!=0]\n","\n","        tags =  [line.split('\\t')[1] for line in lines]\n","        words = [line.split('\\t')[0] for line in lines]\n","\n","        word, tag = [], []\n","        for char, t in zip(words, tags):\n","            if char != '。':\n","                word.append(char)\n","                tag.append(t)\n","            else:\n","                if len(word) > MAX_LEN:\n","                  self.sents.append(['[CLS]'] + word[:MAX_LEN] + ['[SEP]'])\n","                  self.tags_li.append(['[CLS]'] + tag[:MAX_LEN] + ['[SEP]'])\n","                else:\n","                  self.sents.append(['[CLS]'] + word + ['[SEP]'])\n","                  self.tags_li.append(['[CLS]'] + tag + ['[SEP]'])\n","                word, tag = [], []\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx]\n","        token_ids = tokenizer.convert_tokens_to_ids(words)\n","        laebl_ids = [tag2idx[tag] for tag in tags]\n","        seqlen = len(laebl_ids)\n","        return token_ids, laebl_ids, seqlen\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","def PadBatch(batch):#[Pad]\n","    maxlen = max([i[2] for i in batch])\n","    token_tensors = torch.LongTensor([i[0] + [0] * (maxlen - len(i[0])) for i in batch])\n","    label_tensors = torch.LongTensor([i[1] + [0] * (maxlen - len(i[1])) for i in batch])\n","    # mask = (token_tensors > 0)\n","    mask = (token_tensors > 0).to(torch.bool)\n","    return token_tensors, label_tensors, mask"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","    Simple, minimal implementation of Mamba in one file of PyTorch.\n","\n","    Suggest reading the following before/while reading the code:\n","        [1] Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Albert Gu and Tri Dao)\n","            https://arxiv.org/abs/2312.00752\n","        [2] The Annotated S4 (Sasha Rush and Sidd Karamcheti)\n","            https://srush.github.io/annotated-s4\n","\n","    Glossary:\n","        b: batch size                       (`B` in Mamba paper [1] Algorithm 2)\n","        l: sequence length                  (`L` in [1] Algorithm 2)\n","        d or d_model: hidden dim\n","        n or d_state: latent state dim      (`N` in [1] Algorithm 2)\n","        expand: expansion factor            (`E` in [1] Section 3.4)\n","        d_in or d_inner: d * expand         (`D` in [1] Algorithm 2)\n","        A, B, C, D: state space parameters  (See any state space representation formula)\n","                                            (B, C are input-dependent (aka selective, a key innovation in Mamba); A, D are not)\n","        Δ or delta: input-dependent step size\n","        dt_rank: rank of Δ                  (See [1] Section 3.6 \"Parameterization of ∆\")\n","\"\"\""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# 使用dataclass裝飾器自動產生初始化方法和類別的字串表示方法\n","@dataclass\n","class ModelArgs:\n","    # @dataclass 會自動為這個類別產生初始化方法和代表類別的字串形式的方法\n","    d_model: int # 定義模型的隱藏層維度\n","    n_layer: int # 定義模型的層數\n","    vocab_size: int # 定義詞彙表的大小\n","    d_state: int = 16 # 定義狀態空間的維度，預設為16\n","    expand: int = 2 # 定義擴展因子，預設為2\n","    dt_rank: Union[int, str] = 'auto' # 定義輸入依賴步長Δ的秩，'auto'表示自動設定\n","    d_conv: int = 4 # 定義卷積核的維度，預設為4\n","    pad_vocab_size_multiple: int = 8 # 定義詞彙表大小的最小公倍數，預設為8\n","    conv_bias: bool = True # 定義卷積層是否使用偏壓項\n","    bias: bool = False # 定義其他層（如線性層）是否使用偏移項\n","\n","    def __post_init__(self):\n","    # 在__init__後自動被調用，用於執行初始化之後的額外設定或驗證\n","    # 計算內部維度，即擴展後的維度\n","        self.d_inner = int(self.expand * self.d_model)\n","\n","        if self.dt_rank == 'auto':# 如果dt_rank未指定，則自動計算設置\n","        # 根據隱藏層維度自動計算Δ的秩\n","            self.dt_rank = math.ceil(self.d_model / 16)\n","        # 確保vocab_size是pad_vocab_size_multiple的倍數\n","        # 如果不是，請調整為最近的倍數\n","        if self.vocab_size % self.pad_vocab_size_multiple != 0:\n","            self.vocab_size += (self.pad_vocab_size_multiple - self.vocab_size % self.pad_vocab_size_multiple)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["class MambaBlock(nn.Module):\n","    def __init__(self, args: ModelArgs):\n","        \"\"\"A single Mamba block, as described in Figure 3 in Section 3.4 in the Mamba paper [1].\"\"\"\n","        super().__init__()\n","        # 儲存模型參數\n","        self.args = args\n","        # 輸入線性變換層\n","        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n","\n","        # 創建了一個所謂的“深度卷積”，其中每個輸入通道被單獨卷積到每個輸出通道。\n","        # 這意味著每個輸出通道的結果是透過僅與一個輸入通道卷積而得到的。\n","        self.conv1d = nn.Conv1d(\n","        in_channels=args.d_inner,\n","        out_channels=args.d_inner,\n","        bias=args.conv_bias,\n","        kernel_size=args.d_conv,\n","        groups=args.d_inner,\n","        padding=args.d_conv - 1,\n","        )\n","\n","        # x_proj takes in `x` and outputs the input-specific Δ, B, C\n","        # 將輸入x對應到狀態空間模型的參數Δ、B和C\n","        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.d_state * 2, bias=False)\n","\n","        # dt_proj projects Δ from dt_rank to d_in\n","        # 將Δ從args.dt_rank維度映射到args.d_inner維度\n","        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n","\n","        # 建立一個重複的序列，用於初始化狀態空間模型的矩陣A\n","        # n->dxn\n","        A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n","        # 將矩陣A的對數值當作可訓練參數來儲存\n","        self.A_log = nn.Parameter(torch.log(A))\n","        # 初始化矩陣D為全1的可訓練參數\n","        self.D = nn.Parameter(torch.ones(args.d_inner))\n","        # 輸出線性變換層\n","        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n","\n","    def forward(self, x):\n","        \"\"\"\n","            MambaBlock的前向傳播函數，與Mamba論文圖3 Section 3.4相同.\n","\n","            Args:\n","            x: shape (b, l, d) (See Glossary at top for definitions of b, l, d_in, n...)\n","\n","            Returns:\n","            output: shape (b, l, d)\n","\n","            Official Implementation:\n","            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n","            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n","        \"\"\"\n","        # 取得輸入x的維度\n","        # batchsize,seq_len,dim\n","        (b, l, d) = x.shape # 取得輸入x的維度\n","        # 應用輸入線性變換\n","        x_and_res = self.in_proj(x) # shape (b, l, 2 * d_in)\n","        # 將變換後的輸出分為兩部分x和res。\n","        # 所得的x分為兩個部分，一部分x繼續用於後續變換，產生所需的參數，res用於殘差部分\n","        (x, res) = x_and_res.split(split_size=[self.args.d_inner, self.args.d_inner], dim=-1)\n","        # 調整x的形狀\n","        x = rearrange(x, 'b l d_in -> b d_in l')\n","        # 施加深度卷積，然後截取前l個輸出\n","        x = self.conv1d(x)[:, :, :l]\n","        # 再次調整x的形狀\n","        x = rearrange(x, 'b d_in l -> b l d_in')\n","        # 應用SiLU激活函數\n","        x = F.silu(x)\n","        # 運行狀態空間模型\n","        y = self.ssm(x)\n","        # 將res的SiLU活化結果與y相乘\n","        y = y * F.silu(res)\n","        # 應用輸出線性變換\n","        output = self.out_proj(y)\n","        # 回傳輸出結果\n","        return output\n","    \n","    def ssm(self, x):\n","        \"\"\"\n","            運行狀態空間模型，參考Mamba論文 Section 3.2與註釋[2]:\n","            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n","            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n","\n","            Args:\n","            x: shape (b, l, d_in) (See Glossary at top for definitions of b, l, d_in, n...)\n","\n","            Returns:\n","            output: shape (b, l, d_in)\n","\n","            Official Implementation:\n","            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n","        \"\"\"\n","        # 取得A_log的維度\n","        # A在初始化時候經過如下賦值：\n","        # A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n","        # self.A_log = nn.Parameter(torch.log(A))\n","        # （args.d_inner, args.d_state）\n","        (d_in, n) = self.A_log.shape # 取得A_log的維度\n","\n","        # 計算 ∆ A B C D, 這些屬於狀態空間參數.\n","        # A, D 是 與輸入無關的 (見Mamba論文Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n","        # ∆, B, C 與輸入有關(這是與線性是不變模型S4最大的不同,\n","        # 也是為什麼Mamba被稱為 “選擇性” 狀態空間的原因)\n","\n","        # 計算矩陣A\n","        A = -torch.exp(self.A_log.float()) # shape (d_in, n)\n","        # 取D的值\n","        D = self.D.float()\n","\n","        # 應用x的投影變換\n","        # ( b,l,d_in) -> (b, l, dt_rank + 2*n)\n","        x_dbl = self.x_proj(x) # (b, l, dt_rank + 2*n)\n","\n","        # 分割delta, B, C\n","        # delta: (b, l, dt_rank). B, C: (b, l, n)\n","        (delta, B, C) = x_dbl.split(split_size=[self.args.dt_rank, n, n], dim=-1)\n","        # 應用dt_proj併計算delta\n","        delta = F.softplus(self.dt_proj(delta)) # (b, l, d_in)\n","        # 應用選擇性掃描演算法\n","        y = self.selective_scan(x, delta, A, B, C, D)\n","        return y\n","\n","\n","    def selective_scan(self, u, delta, A, B, C, D):\n","        \"\"\"\n","            執行選擇性掃描演算法，參考Mamba論文[1] Section 2和註釋[2]. See:\n","            - Section 2 State Space Models in the Mamba paper [1]\n","            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n","            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n","\n","            經典的離散狀態空間公式:\n","            x(t + 1) = Ax(t) + Bu(t)\n","            y(t) = Cx(t) + Du(t)\n","            除了B和C (以及step size delta用於離散化) 與輸入x(t)相關.\n","\n","            參數:\n","            u: shape (b, l, d_in)\n","            delta: shape (b, l, d_in)\n","            A: shape (d_in, n)\n","            B: shape (b, l, n)\n","            C: shape (b, l, n)\n","            D: shape (d_in,)\n","\n","            過程概述：\n","\n","            Returns:\n","            output: shape (b, l, d_in)\n","\n","            Official Implementation:\n","            selective_scan_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L86\n","            Note: I refactored some parts out of `selective_scan_ref` out, so the functionality doesn't match exactly.\n","        \"\"\"\n","        # 取得輸入u的維度\n","        (b, l, d_in) = u.shape\n","        # 取得矩陣A的列數\n","        n = A.shape[1] # A: shape (d_in, n)\n","\n","        # 離散化連續參數(A, B)\n","        # - A 使用 zero-order hold (ZOH) 離散化 (see Section 2 Equation 4 in the Mamba paper [1])\n","        # - B is 使用一個簡化的Euler discretization而不是ZOH.根據作者的討論:\n","        # \"A is the more important term and the performance doesn't change much with the simplification on B\"\n","\n","        # 計算離散化的A\n","        # 將delta和A進行點乘，將A沿著delta的最後一個維度進行廣播，然後執行逐元素乘法\n","        # A:(d_in, n),delta:(b, l, d_in)\n","        # A廣播拓展->(b,l,d_in, n)，deltaA對應原論文中的A_bar\n","        deltaA = torch.exp(einsum(delta, A, 'b l d_in, d_in n -> b l d_in n'))\n","        # delta、B和u,這個計算和原始論文不同\n","        deltaB_u = einsum(delta, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n')\n","\n","        # Perform selective scan (see scan_SSM() in The Annotated S4 [2])\n","        # Note that the below is sequential, while the official implementation does a much faster parallel scan that\n","        # is additionally hardware-aware (like FlashAttention).\n","        # 執行選擇性掃描,初始化狀態x為零\n","        x = torch.zeros((b, d_in, n), device=deltaA.device)\n","        # 初始化輸出列表ys\n","        ys = []\n","        for i in range(l):\n","            # 更新狀態x\n","            # deltaA:((b,l,d_in, n)\n","            # deltaB_u:( b,l,d_in,n)\n","            # x:\n","            x = deltaA[:, i] * x + deltaB_u[:, i]\n","            # 計算輸出y\n","            y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')\n","            # 將輸出y加入到列表ys中\n","            ys.append(y)\n","        # 將清單ys堆疊成張量y\n","        y = torch.stack(ys, dim=1) # shape (b, l, d_in)\n","        # 將輸入u乘以D並加到輸出y上\n","        y = y + u * D\n","\n","        return y"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["class ResidualBlock(nn.Module):\n","    def __init__(self, args: ModelArgs):\n","        \"\"\"Simple block wrapping Mamba block with normalization and residual connection.\"\"\"\n","        super().__init__()\n","        # 保存傳入的ModelArgs對象，包含模型的配置參數\n","        self.args = args\n","        # 建立一個MambaBlock，它是這個殘差區塊的核心元件\n","        self.mixer = MambaBlock(args)\n","        # 建立一個RMSNorm歸一化模組，用於歸一化操作\n","        self.norm = RMSNorm(args.d_model)\n","\n","\n","    def forward(self, x):\n","        \"\"\"\n","            Args:\n","            x: shape (b, l, d) (See Glossary at top for definitions of b, l, d_in, n...)\n","            x (Tensor): 輸入張量，形狀為(batch_size, sequence_length, hidden_​​size)\n","            Returns:\n","            output: shape (b, l, d)\n","            輸出張量，形狀與輸入相同\n","            Official Implementation:\n","            Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297\n","\n","            Note: the official repo chains residual blocks that look like\n","            [加 -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> ...\n","            where the first Add is a no-op. This is purely for performance reasons as this\n","            allows them to fuse the Add->Norm.\n","\n","            We instead implement our blocks as the more familiar, simpler, and numerically equivalent\n","            [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> ....\n","        \"\"\"\n","        # 應用歸一化和MambaBlock，然後與輸入x進行殘差連接\n","        output = self.mixer(self.norm(x)) + x\n","\n","        return output"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["class RMSNorm(nn.Module):\n","    \"\"\"\n","        初始化RMSNorm模組，該模組實現了基於均方根的歸一化操作。\n","\n","        參數:\n","        d_model (int): 模型的特徵維度。\n","        eps (float, 可選): 為了避免除以零，加到分母中的一個小的常數。\n","    \"\"\"\n","    def __init__(self, d_model: int, eps: float = 1e-5):\n","        super().__init__()\n","        self.eps = eps# 保存輸入的eps值，用於數值穩定性。\n","        # 建立一個可訓練的權重參數，初始值為全1，維度與輸入特徵維度d_model相同。\n","        self.weight = nn.Parameter(torch.ones(d_model))\n","\n","\n","    def forward(self, x):\n","        \"\"\"\n","            計算輸入x的均方根值，用於後續的歸一化操作。\n","            x.pow(2) 計算x中每個元素的平方。\n","            mean(-1, keepdim=True) 將x的最後一個維度（特徵維度）平方和求平均，保持維度以便進行廣播操作。\n","            torch.rsqrt 對求得的平均值取倒數和平方根，得到每個特徵的均方根值的逆。\n","            + self.eps 加入一個小的常數eps以保持數值穩定性，防止除以零的情況發生。\n","            x * ... * self.weight 將輸入x與計算得到的歸一化因子和可訓練的權重相乘，得到最終的歸一化輸出。\n","        \"\"\"\n","        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n","\n","        return output"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["class Mamba(nn.Module):\n","    def __init__(self, args: ModelArgs, num_class):\n","        \"\"\"Full Mamba model.\"\"\"\n","        super().__init__()\n","        # 儲存傳入的ModelArgs對象，包含模型的配置參數\n","        self.args = args\n","        # 建立一個嵌入層，將詞彙表中的單字轉換為對應的向量表示\n","        self.embedding = nn.Embedding(args.vocab_size, args.d_model)\n","        # 建立一個包含多個殘差塊的模組列表，殘差塊的數量等於模型層數\n","        self.layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n","        # 建立一個RMSNorm模組，用於歸一化操作\n","        self.norm_f = RMSNorm(args.d_model)\n","        # 建立一個線性層，用於最終的輸出，將隱藏層的輸出對應回詞彙表的大小\n","        self.lm_head = nn.Linear(args.d_model, num_class, bias=False)\n","        # 將線性層的輸出權重與嵌入層的權重綁定，這是權重共享的一種形式，有助於減少參數數量並可能提高模型的泛化能力\n","        self.lm_head.weight = self.embedding.weight # Tie output projection to embedding weights. # See \"Weight Tying\" paper\n","    \n","\n","    def forward(self, input_ids):\n","        \"\"\"\n","            Args:\n","            input_ids (long tensor): shape (b, l) (See Glossary at top for definitions of b, l, d_in, n...)\n","\n","            Returns:\n","            logits: shape (b, l, vocab_size)\n","\n","            Official Implementation:\n","            class MambaLMHeadModel, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L173\n","        \"\"\"\n","        # 將輸入ID轉換為向量表示\n","        x = self.embedding(input_ids)\n","        # 遍歷所有的殘差塊，並應用它們\n","        for layer in self.layers:\n","            x = layer(x)\n","        # 應用歸一化操作\n","        x = self.norm_f(x)\n","        # 透過線性層得到最終的logits輸出\n","        logits = self.lm_head(x)\n","        # 傳回模型的輸出\n","        return logits\n","\n","\n","    @staticmethod\n","    def from_pretrained(pretrained_model_name: str):\n","        \"\"\"\n","            Load pretrained weights from HuggingFace into model.\n","\n","            Args:\n","            pretrained_model_name: One of\n","            * 'state-spaces/mamba-2.8b-slimpj'\n","            * 'state-spaces/mamba-2.8b'\n","            * 'state-spaces/mamba-1.4b'\n","            * 'state-spaces/mamba-790m'\n","            * 'state-spaces/mamba-370m'\n","            * 'state-spaces/mamba-130m'\n","\n","            Returns:\n","            model: Mamba model with weights loaded\n","        \"\"\"\n","        from transformers.utils import WEIGHTS_NAME, CONFIG_NAME\n","        from transformers.utils.hub import cached_file\n","\n","        def load_config_hf(model_name):\n","            resolved_archive_file = cached_file(model_name, CONFIG_NAME, _raise_exceptions_for_missing_entries=False)\n","            return json.load(open(resolved_archive_file))\n","\n","\n","        def load_state_dict_hf(model_name, device=None, dtype=None):\n","            resolved_archive_file = cached_file(model_name, WEIGHTS_NAME, _raise_exceptions_for_missing_entries=False)\n","            return torch.load(resolved_archive_file, weights_only=True, map_location='cpu', mmap=True)\n","\n","        config_data = load_config_hf(pretrained_model_name)\n","        args = ModelArgs(\n","        d_model=config_data['d_model'],\n","        n_layer=config_data['n_layer'],\n","        vocab_size=config_data['vocab_size']\n","        )\n","        model = Mamba(args)\n","\n","        state_dict = load_state_dict_hf(pretrained_model_name)\n","        new_state_dict = {}\n","        for key in state_dict:\n","            new_key = key.replace('backbone.', '')\n","            new_state_dict[new_key] = state_dict[key]\n","        model.load_state_dict(new_state_dict)\n","\n","        return model"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["class MambaCRF(nn.Module):\n","    def __init__(self, args: ModelArgs, num_class):\n","        \"\"\"Full Mamba + CRF model.\"\"\"\n","        super().__init__()\n","        # 儲存傳入的ModelArgs對象，包含模型的配置參數\n","        self.args = args\n","        # 建立一個嵌入層，將詞彙表中的單字轉換為對應的向量表示\n","        self.embedding = nn.Embedding(args.vocab_size, args.d_model)\n","        # 建立一個包含多個殘差塊的模組列表，殘差塊的數量等於模型層數\n","        self.layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n","        # 建立一個RMSNorm模組，用於歸一化操作\n","        self.norm_f = RMSNorm(args.d_model)\n","        # 建立一個線性層，用於最終的輸出，將隱藏層的輸出對應回詞彙表的大小\n","        self.lm_head = nn.Linear(args.d_model, num_class)\n","        # self.lm_head = nn.Linear(args.d_model, num_class, bias=False)\n","        # 將線性層的輸出權重與嵌入層的權重綁定，這是權重共享的一種形式，有助於減少參數數量並可能提高模型的泛化能力\n","        # self.lm_head.weight = self.embedding.weight # Tie output projection to embedding weights. # See \"Weight Tying\" paper\n","        # 將CRF加入\n","        self.crf = CRF(num_class, batch_first=True)  \n","        # self.crf = CRF(len(tag2idx), batch_first=True) \n","    \n","\n","    def forward(self, input_ids, tags, mask, is_test=False):\n","        \"\"\"\n","            Args:\n","            input_ids (long tensor): shape (b, l) (See Glossary at top for definitions of b, l, d_in, n...)\n","\n","            Returns:\n","            logits: shape (b, l, vocab_size)\n","\n","            Official Implementation:\n","            class MambaLMHeadModel, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L173\n","        \"\"\"\n","        # 將輸入ID轉換為向量表示\n","        x = self.embedding(input_ids)\n","        # 遍歷所有的殘差塊，並應用它們\n","        for layer in self.layers:\n","            x = layer(x)\n","        # 應用歸一化操作\n","        x = self.norm_f(x)\n","        # 透過線性層得到最終的logits輸出\n","        logits = self.lm_head(x)\n","        # 傳回模型的輸出\n","\n","        if not is_test: # Training，return loss\n","            loss=-self.crf.forward(logits, tags, mask, reduction='mean')\n","            return loss\n","        else: # Testing，return decoding\n","            decode=self.crf.decode(logits, mask)\n","            return decode\n","\n","\n","    @staticmethod\n","    def from_pretrained(pretrained_model_name: str):\n","        \"\"\"\n","            Load pretrained weights from HuggingFace into model.\n","\n","            Args:\n","            pretrained_model_name: One of\n","            * 'state-spaces/mamba-2.8b-slimpj'\n","            * 'state-spaces/mamba-2.8b'\n","            * 'state-spaces/mamba-1.4b'\n","            * 'state-spaces/mamba-790m'\n","            * 'state-spaces/mamba-370m'\n","            * 'state-spaces/mamba-130m'\n","\n","            Returns:\n","            model: Mamba model with weights loaded\n","        \"\"\"\n","        from transformers.utils import WEIGHTS_NAME, CONFIG_NAME\n","        from transformers.utils.hub import cached_file\n","\n","        def load_config_hf(model_name):\n","            resolved_archive_file = cached_file(model_name, CONFIG_NAME, _raise_exceptions_for_missing_entries=False)\n","            return json.load(open(resolved_archive_file))\n","\n","\n","        def load_state_dict_hf(model_name, device=None, dtype=None):\n","            resolved_archive_file = cached_file(model_name, WEIGHTS_NAME, _raise_exceptions_for_missing_entries=False)\n","            return torch.load(resolved_archive_file, weights_only=True, map_location='cpu', mmap=True)\n","\n","        config_data = load_config_hf(pretrained_model_name)\n","        args = ModelArgs(\n","        d_model=config_data['d_model'],\n","        n_layer=config_data['n_layer'],\n","        vocab_size=config_data['vocab_size']\n","        )\n","        model = Mamba(args)\n","\n","        state_dict = load_state_dict_hf(pretrained_model_name)\n","        new_state_dict = {}\n","        for key in state_dict:\n","            new_key = key.replace('backbone.', '')\n","            new_state_dict[new_key] = state_dict[key]\n","        model.load_state_dict(new_state_dict)\n","\n","        return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# args = ModelArgs(d_model=320, n_layer=3, vocab_size=30522)\n","args = ModelArgs(d_model=320, n_layer=3, vocab_size=10000)\n","\n","# model = Mamba(args, len(labels))\n","model = MambaCRF(args, len(tag2idx))\n","model"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["mamba_model parameters: 5,235,208\n"]}],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","def format_parameters(num_params):\n","    return \"{:,}\".format(num_params)\n","\n","model_params = count_parameters(model)\n","print(f\"mamba_model parameters: {format_parameters(model_params)}\")"]},{"cell_type":"markdown","metadata":{"id":"dYMEg1XAAex5"},"source":["# Main.ipynb"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"ufTXiTWhR8ra"},"outputs":[],"source":["def train(e, model, iterator, optimizer, scheduler, device):\n","    start_time = time.time()  # Record the start time\n","\n","    model.train().to(device)\n","    losses = 0.0\n","    step = 0\n","    for i, batch in enumerate(iterator):\n","        step += 1\n","        x, y, z = batch\n","        x = x.to(device)\n","        y = y.to(device)\n","        z = z.to(device)\n","\n","        loss = model(x, y, z)\n","        losses += loss.item()\n","        \"\"\" Gradient Accumulation \"\"\"\n","        '''\n","          full_loss = loss / 2                            # normalize loss\n","          full_loss.backward()                            # backward and accumulate gradient\n","          if step % 2 == 0:\n","              optimizer.step()                            # update optimizer\n","              scheduler.step()                            # update scheduler\n","              optimizer.zero_grad()                       # clear gradient\n","        '''\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","    end_time = time.time()  # Record the end time\n","    epoch_time = end_time - start_time\n","\n","    print(\"Epoch: {}, Loss:{:.4f}, epoch_time:{:.2f} sec\".format(e, losses/step, epoch_time))\n","\n","def validate(e, model, iterator, device):\n","    start_time = time.time()  # Record the start time\n","\n","    model.eval()\n","    Y, Y_hat = [], []\n","    losses = 0\n","    step = 0\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            step += 1\n","\n","            x, y, z = batch\n","            x = x.to(device)\n","            y = y.to(device)\n","            z = z.to(device)\n","\n","            y_hat = model(x, y, z, is_test=True)\n","\n","            loss = model(x, y, z)\n","            losses += loss.item()\n","            # Save prediction\n","            for j in y_hat:\n","              Y_hat.extend(j)\n","            # Save labels\n","            mask = (z==1)\n","            y_orig = torch.masked_select(y, mask)\n","            Y.append(y_orig.cpu())\n","\n","    Y = torch.cat(Y, dim=0).numpy()\n","    Y_hat = np.array(Y_hat)\n","    acc = (Y_hat == Y).mean()*100\n","\n","    end_time = time.time()  # Record the end time\n","    epoch_time = end_time - start_time\n","\n","    print(\"Epoch: {}, Val Loss:{:.4f}, Val Acc:{:.3f}, epoch_time:{:.2f} sec\".format(e, losses/step, acc, epoch_time))\n","    return model, losses/step, acc\n","\n","def test(model, iterator, device):\n","    model.eval()\n","    Y, Y_hat = [], []\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            x, y, z = batch\n","            x = x.to(device)\n","            z = z.to(device)\n","            y_hat = model(x, y, z, is_test=True)\n","            # Save prediction\n","            for j in y_hat:\n","              Y_hat.extend(j)\n","            # Save labels\n","            mask = (z==1).cpu()\n","            y_orig = torch.masked_select(y, mask)\n","            Y.append(y_orig)\n","\n","    Y = torch.cat(Y, dim=0).numpy()\n","    y_true = [idx2tag[i] for i in Y]\n","    y_pred = [idx2tag[i] for i in Y_hat]\n","\n","    return y_true, y_pred\n","\n","# def main(batch_size=64, lr=0.001, n_epochs=10, trainset=\"data/train.txt\", validset=\"data/msra_eval.txt\", testset=\"data/test.txt\"):\n","def main(batch_size=batch_size, lr=0.001, n_epochs=10, trainset=train_path, validset=valid_path, testset=test_path):\n","\n","    best_model = None\n","    _best_val_loss = 1e18\n","    _best_val_acc = 1e-18\n","\n","    args = ModelArgs(d_model=768, n_layer=12, vocab_size=10000)\n","    model = MambaCRF(args, len(tag2idx))\n","    \n","    print('Initial model Done.')\n","    train_dataset = NerDataset(trainset)\n","    eval_dataset = NerDataset(validset)\n","    test_dataset = NerDataset(testset)\n","    print('Load Data Done.')\n","\n","    train_iter = data.DataLoader(dataset=train_dataset,\n","                                 batch_size=batch_size,\n","                                 shuffle=True,\n","                                 num_workers=4,\n","                                 collate_fn=PadBatch)\n","\n","    eval_iter = data.DataLoader(dataset=eval_dataset,\n","                                 batch_size=batch_size,\n","                                 shuffle=False,\n","                                 num_workers=4,\n","                                 collate_fn=PadBatch)\n","\n","    test_iter = data.DataLoader(dataset=test_dataset,\n","                                batch_size=batch_size,\n","                                shuffle=False,\n","                                num_workers=4,\n","                                collate_fn=PadBatch)\n","\n","    #optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=0.01)\n","    optimizer = AdamW(model.parameters(), lr=lr, eps=1e-6)\n","\n","    # Warmup\n","    len_dataset = len(train_dataset)\n","    epoch = n_epochs\n","    batch_size = batch_size\n","    total_steps = (len_dataset // batch_size) * epoch if len_dataset % batch_size == 0 else (len_dataset // batch_size + 1) * epoch\n","\n","    warm_up_ratio = 0.1 # Define 10% steps\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warm_up_ratio * total_steps, num_training_steps = total_steps)\n","\n","    print('Start Train...,')\n","    for epoch in range(1, n_epochs+1):\n","\n","        train(epoch, model, train_iter, optimizer, scheduler, device)\n","        candidate_model, loss, acc = validate(epoch, model, eval_iter, device)\n","\n","        save_dir = '/home/yenling/Code In Lunix/mamba/Save Model'\n","        os.makedirs(save_dir, exist_ok=True)\n","\n","        if loss < _best_val_loss and acc > _best_val_acc:\n","          best_model = candidate_model\n","          _best_val_loss = loss\n","          _best_val_acc = acc\n","\n","          save_path = os.path.join(save_dir, f'Mamba + CRF with FinetuneBlock best_model_epoch_{epoch}_loss_{loss:.4f}_acc_{acc:.4f}.pt')\n","          torch.save(best_model.state_dict(), save_path)\n","          print(f\"Best model saved at epoch {epoch} with val_loss: {loss:.4f} and val_acc: {acc:.4f} to {save_path}\")\n","\n","        print(\"=============================================\")\n","\n","    y_test, y_pred = test(best_model, test_iter, device)\n","    print(metrics.classification_report(y_test, y_pred, labels=labels, digits=3))\n","    print(metrics.confusion_matrix(y_test, y_pred, labels=labels))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["main(n_epochs=5)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
