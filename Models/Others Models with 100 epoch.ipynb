{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12639,"status":"ok","timestamp":1721963002838,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"iXXjuhI5PdOs"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchcrf import CRF\n","from peft import  LoraConfig, get_peft_model\n","from transformers import BertTokenizerFast, BertModel, AutoModel,  AutoTokenizer, AutoModelForCausalLM"]},{"cell_type":"markdown","metadata":{"id":"CgEYhLfOAp4a"},"source":["# Model.ipynb"]},{"cell_type":"markdown","metadata":{"id":"8rrJGEiXFMbM"},"source":["## LoRATrainBert_BiLSTM_CRF"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1721963002838,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"YF3E6eaAFME5"},"outputs":[],"source":["class LoRATrainBert_BiLSTM_CRF(nn.Module):\n","\n","    def __init__(self, tag_to_ix, embedding_dim=21128, hidden_dim=256):\n","        super(LoRATrainBert_BiLSTM_CRF, self).__init__()\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","        self.hidden_dim = hidden_dim\n","        self.embedding_dim = embedding_dim\n","\n","        self.bert = AutoModelForCausalLM.from_pretrained('UJForSchool/Bert_base_chinese_LoRA', is_decoder=True, return_dict=False)\n","        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim//2,\n","                            num_layers=2, bidirectional=True, batch_first=True)\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.linear = nn.Linear(hidden_dim, self.tagset_size)\n","        self.crf = CRF(self.tagset_size, batch_first=True)\n","\n","    def _get_features(self, sentence):\n","        with torch.no_grad():\n","          embeds, _  = self.bert(sentence)\n","        enc, _ = self.lstm(embeds)\n","        enc = self.dropout(enc)\n","        feats = self.linear(enc)\n","        return feats\n","\n","    def forward(self, sentence, tags, mask, is_test=False):\n","        emissions = self._get_features(sentence)\n","        if not is_test: # Training，return loss\n","            loss=-self.crf.forward(emissions, tags, mask, reduction='mean')\n","            return loss\n","        else: # Testing，return decoding\n","            decode=self.crf.decode(emissions, mask)\n","            return decode"]},{"cell_type":"markdown","metadata":{"id":"64xmh9IyRV6L"},"source":["## Lamma_BiLSTM_CRF"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1721963002838,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"v75WBKl9Avjt"},"outputs":[],"source":["class Lamma_BiLSTM_CRF(nn.Module):\n","\n","    def __init__(self, tag_to_ix, embedding_dim=768, hidden_dim=256):\n","        super(Lamma_BiLSTM_CRF, self).__init__()\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","        self.hidden_dim = hidden_dim\n","        self.embedding_dim = embedding_dim\n","\n","        self.lamma = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", use_auth_token=True)\n","        # self.lamma = AutoModelForCausalLM.from_pretrained(\"hfl/chinese-llama-2-lora-7b-64k\")\n","\n","        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim//2,\n","                            num_layers=2, bidirectional=True, batch_first=True)\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.linear = nn.Linear(hidden_dim, self.tagset_size)\n","        self.crf = CRF(self.tagset_size, batch_first=True)\n","\n","    def _get_features(self, sentence):\n","        with torch.no_grad():\n","            embeds = self.lamma(sentence)['last_hidden_state']\n","        enc, _ = self.lstm(embeds)\n","        enc = self.dropout(enc)\n","        feats = self.linear(enc)\n","        return feats\n","\n","    def forward(self, sentence, tags, mask, is_test=False):\n","        emissions = self._get_features(sentence)\n","        if not is_test:  # Training, return loss\n","            loss = -self.crf.forward(emissions, tags, mask, reduction='mean')\n","            return loss\n","        else:  # Testing, return decoding\n","            decode = self.crf.decode(emissions, mask)\n","            return decode"]},{"cell_type":"markdown","metadata":{"id":"FwslzXP8RQji"},"source":["## BERTWWM_BiLSTM_CRF"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1721963002838,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"3-iTcaJ3RPgS"},"outputs":[],"source":["class BERTWWM_BiLSTM_CRF(nn.Module):\n","    def __init__(self, tag_to_ix, embedding_dim=768, hidden_dim=256):\n","        super(BERTWWM_BiLSTM_CRF, self).__init__()\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","        self.hidden_dim = hidden_dim\n","        self.embedding_dim = embedding_dim\n","\n","        config = AutoTokenizer.from_pretrained(\"hfl/chinese-bert-wwm\", output_hidden_states=True)\n","        self.bert = AutoModel.from_pretrained(\"hfl/chinese-bert-wwm\", config=config)\n","\n","        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim//2,\n","                            num_layers=2, bidirectional=True, batch_first=True)\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.linear = nn.Linear(hidden_dim, self.tagset_size)\n","        self.crf = CRF(self.tagset_size, batch_first=True)\n","\n","    def _get_features(self, sentence):\n","        with torch.no_grad():\n","            embeds = self.bert(sentence)['last_hidden_state']\n","        enc, _ = self.lstm(embeds)\n","        enc = self.dropout(enc)\n","        feats = self.linear(enc)\n","        return feats\n","\n","    def forward(self, sentence, tags, mask, is_test=False):\n","        emissions = self._get_features(sentence)\n","        if not is_test:  # Training, return loss\n","            loss = -self.crf.forward(emissions, tags, mask, reduction='mean')\n","            return loss\n","        else:  # Testing, return decoding\n","            decode = self.crf.decode(emissions, mask)\n","            return decode"]},{"cell_type":"markdown","metadata":{"id":"03mWv4aRRMzr"},"source":["## MacBERT_BiLSTM_CRF"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1721963002839,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"ukLAEHajRLYz"},"outputs":[],"source":["class MacBERT_BiLSTM_CRF(nn.Module):\n","\n","    def __init__(self, tag_to_ix, embedding_dim=768, hidden_dim=256):\n","        super(MacBERT_BiLSTM_CRF, self).__init__()\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","        self.hidden_dim = hidden_dim\n","        self.embedding_dim = embedding_dim\n","\n","        config = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\", output_hidden_states=True)\n","        self.bert = AutoModel.from_pretrained(\"hfl/chinese-macbert-base\", config=config)\n","\n","        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim//2,\n","                            num_layers=2, bidirectional=True, batch_first=True)\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.linear = nn.Linear(hidden_dim, self.tagset_size)\n","        self.crf = CRF(self.tagset_size, batch_first=True)\n","\n","    def _get_features(self, sentence):\n","        with torch.no_grad():\n","            embeds = self.bert(sentence)['last_hidden_state']\n","        enc, _ = self.lstm(embeds)\n","        enc = self.dropout(enc)\n","        feats = self.linear(enc)\n","        return feats\n","\n","    def forward(self, sentence, tags, mask, is_test=False):\n","        emissions = self._get_features(sentence)\n","        if not is_test:  # Training, return loss\n","            loss = -self.crf.forward(emissions, tags, mask, reduction='mean')\n","            return loss\n","        else:  # Testing, return decoding\n","            decode = self.crf.decode(emissions, mask)\n","            return decode"]},{"cell_type":"markdown","metadata":{"id":"GXVaC1vTRIo-"},"source":["## Bert_LoRA_BiLSTM_CRF"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1721963002839,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"jtd7zAirRFna"},"outputs":[],"source":["class Bert_LoRA_BiLSTM_CRF(nn.Module):\n","    def __init__(self, tag_to_ix, num_layers_to_finetune, r, lora_alpha, lora_dropout, embedding_dim=768, hidden_dim=256):\n","        super(Bert_LoRA_BiLSTM_CRF, self).__init__()\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","        self.num_layers_to_finetune = num_layers_to_finetune\n","        self.r = r\n","        self.lora_alpha = lora_alpha\n","        self.lora_dropout = lora_dropout\n","        self.hidden_dim = hidden_dim\n","        self.embedding_dim = embedding_dim\n","\n","\n","        # 创建并加载具有隐藏状态的BERT模型。\n","        self.bert = AutoModel.from_pretrained('bert-base-chinese')\n","        # self.bert = AutoModel.from_pretrained('hfl/chinese-llama-2-lora-7b')\n","\n","        # 微调指定的层。\n","        self.fine_tune_layers()\n","\n","        self.lora_config = LoraConfig(\n","            r = self.r,\n","            lora_alpha = self.lora_alpha,\n","            lora_dropout = self.lora_dropout,\n","            # task_type = None,\n","            # task_type = TaskType.SEQ_CLS, # TypeError: forward() got an unexpected keyword argument 'labels'\n","            # task_type = TaskType.CAUSAL_LM, # TypeError: forward() got an unexpected keyword argument 'labels'\n","            # task_type = TaskType.SEQ_2_SEQ_LM, # TypeError: forward() got an unexpected keyword argument 'decoder_input_ids'\n","            task_type = TaskType.TOKEN_CLS, # TypeError: forward() got an unexpected keyword argument 'labels'\n","            inference_mode = True,\n","        )\n","\n","        self.bert = get_peft_model(self.bert, self.lora_config)\n","\n","        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2,\n","                            num_layers=2, bidirectional=True, batch_first=True)\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.linear = nn.Linear(hidden_dim, self.tagset_size)\n","        self.crf = CRF(self.tagset_size, batch_first=True)\n","\n","    def fine_tune_layers(self):\n","        # 冻结所有层。\n","        for param in self.bert.parameters():\n","            param.requires_grad = False\n","\n","        # # 解冻指定的层。\n","        for i in range(12 - self.num_layers_to_finetune, 12):\n","            for param in self.bert.encoder.layer[i].parameters():\n","                param.requires_grad = True\n","\n","    def _get_features(self, sentence,mask):\n","        # with torch.no_grad():\n","        #   embeds = self.bert(sentence)['last_hidden_state']\n","        # enc, _ = self.lstm(embeds)\n","        # enc = self.dropout(enc)\n","        # feats = self.linear(enc)\n","        with torch.no_grad():\n","           embeds = self.bert(sentence, attention_mask=mask)['last_hidden_state']\n","        enc, _ = self.lstm(embeds)\n","        enc = self.dropout(enc)\n","        feats = self.linear(enc)\n","        return feats\n","\n","    def forward(self, sentence, tags, mask, is_test=False):\n","        emissions = self._get_features(sentence,mask)\n","        if not is_test: # Training，return loss\n","            loss=-self.crf.forward(emissions, tags, mask, reduction='mean')\n","            return loss\n","        else: # Testing，return decoding\n","            decode=self.crf.decode(emissions, mask)\n","            return decode"]},{"cell_type":"markdown","metadata":{"id":"uZfV3E56RBWy"},"source":["## ClinicalDistilBERT_BiLSTM_CRF"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1721963002839,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"H5xoog5URAVa"},"outputs":[],"source":["class ClinicalDistilBERT_BiLSTM_CRF(nn.Module):\n","\n","    def __init__(self, tag_to_ix, embedding_dim=768, hidden_dim=256):\n","        super(ClinicalDistilBERT_BiLSTM_CRF, self).__init__()\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","        self.hidden_dim = hidden_dim\n","        self.embedding_dim = embedding_dim\n","\n","        # Build the DistilBERT model, including outputting all hidden states\n","        config = AutoTokenizer.from_pretrained(\"medicalai/ClinicalBERT\", output_hidden_states=True)\n","        self.bert = AutoModel.from_pretrained(\"medicalai/ClinicalBERT\", config=config)\n","\n","\n","        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim//2,\n","                            num_layers=2, bidirectional=True, batch_first=True)\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.linear = nn.Linear(hidden_dim, self.tagset_size)\n","        self.crf = CRF(self.tagset_size, batch_first=True)\n","\n","    def _get_features(self, sentence):\n","        with torch.no_grad():\n","            embeds = self.bert(sentence)['last_hidden_state']\n","        enc, _ = self.lstm(embeds)\n","        enc = self.dropout(enc)\n","        feats = self.linear(enc)\n","        return feats\n","\n","    def forward(self, sentence, tags, mask, is_test=False):\n","        emissions = self._get_features(sentence)\n","        if not is_test:  # Training, return loss\n","            loss = -self.crf.forward(emissions, tags, mask, reduction='mean')\n","            return loss\n","        else:  # Testing, return decoding\n","            decode = self.crf.decode(emissions, mask)\n","            return decode"]},{"cell_type":"markdown","metadata":{"id":"niwB-6Uuh_yv"},"source":["## Bert_BiLSTM_CRF"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1721963002839,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"QfmLnLTkQr4L"},"outputs":[],"source":["class Bert_BiLSTM_CRF(nn.Module):\n","    def __init__(self, tag_to_ix, embedding_dim=768, hidden_dim=256):\n","        super(Bert_BiLSTM_CRF, self).__init__()\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","        self.hidden_dim = hidden_dim\n","        self.embedding_dim = embedding_dim\n","\n","        # self.bert = BertModel.from_pretrained('bert-base-chinese')\n","        self.bert = BertModel.from_pretrained('bert-base-chinese',return_dict=False)\n","        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim//2,\n","                            num_layers=2, bidirectional=True, batch_first=True)\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.linear = nn.Linear(hidden_dim, self.tagset_size)\n","        self.crf = CRF(self.tagset_size, batch_first=True)\n","\n","    def _get_features(self, sentence):\n","        with torch.no_grad():\n","          embeds, _  = self.bert(sentence)\n","        enc, _ = self.lstm(embeds)\n","        enc = self.dropout(enc)\n","        feats = self.linear(enc)\n","        return feats\n","\n","    def forward(self, sentence, tags, mask, is_test=False):\n","        emissions = self._get_features(sentence)\n","        if not is_test: # Training，return loss\n","            loss=-self.crf.forward(emissions, tags, mask, reduction='mean')\n","            return loss\n","        else: # Testing，return decoding\n","            decode=self.crf.decode(emissions, mask)\n","            return decode"]},{"cell_type":"markdown","metadata":{"id":"qnB1WD4-HBxZ"},"source":["## Bert_CRF"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1721963002839,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"IMLKVIsxHBRr"},"outputs":[],"source":["class Bert_CRF(nn.Module):\n","\n","    # def __init__(self, tag_to_ix, num_layers_to_finetune, embedding_dim=768, hidden_dim=768):\n","    def __init__(self, tag_to_ix, embedding_dim=768, hidden_dim=768):\n","        super(Bert_CRF, self).__init__()\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","        # self.num_layers_to_finetune = num_layers_to_finetune\n","        self.hidden_dim = hidden_dim\n","        self.embedding_dim = embedding_dim\n","\n","        # self.bert = BertModel.from_pretrained('bert-base-chinese')\n","        self.bert = BertModel.from_pretrained('bert-base-chinese',return_dict=False)\n","\n","        # # 微调指定的层。\n","        # self.fine_tune_layers()\n","\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.linear = nn.Linear(self.hidden_dim, self.tagset_size)\n","        self.crf = CRF(self.tagset_size, batch_first=True)\n","\n","    def _get_features(self, sentence):\n","        with torch.no_grad():\n","          embeds, _  = self.bert(sentence)\n","        enc = self.dropout(embeds)\n","        feats = self.linear(enc)\n","        return feats\n","\n","    def forward(self, sentence, tags, mask, is_test=False):\n","        emissions = self._get_features(sentence)\n","        if not is_test: # Training，return loss\n","            loss=-self.crf.forward(emissions, tags, mask, reduction='mean')\n","            return loss\n","        else: # Testing，return decoding\n","            decode=self.crf.decode(emissions, mask)\n","            return decode\n","\n","    # def fine_tune_layers(self):\n","    #     # 冻结所有层。\n","    #     for param in self.bert.parameters():\n","    #         param.requires_grad = False\n","\n","    #     # # 解冻指定的层。\n","    #     for i in range(12 - self.num_layers_to_finetune, 12):\n","    #         for param in self.bert.encoder.layer[i].parameters():\n","    #             param.requires_grad = True"]},{"cell_type":"markdown","metadata":{"id":"8cCmhmTNQstK"},"source":["## Roberta_BiLSTM_CRF"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":465,"status":"ok","timestamp":1721963003302,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"RuDlZwFnh9W4"},"outputs":[],"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","class Roberta_BiLSTM_CRF(nn.Module):\n","    def __init__(self, tag_to_ix, embedding_dim=768, hidden_dim=256):\n","        super(Roberta_BiLSTM_CRF, self).__init__()\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","        self.hidden_dim = hidden_dim\n","        self.embedding_dim = embedding_dim\n","\n","        # Build the DistilBERT model, including outputting all hidden states\n","        config = AutoTokenizer.from_pretrained(\"roberta-base\", output_hidden_states=True)\n","        self.bert = AutoModel.from_pretrained(\"roberta-base\", config=config)\n","\n","\n","        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim//2,\n","                            num_layers=2, bidirectional=True, batch_first=True)\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.linear = nn.Linear(hidden_dim, self.tagset_size)\n","        self.crf = CRF(self.tagset_size, batch_first=True)\n","\n","    def _get_features(self, sentence):\n","        with torch.no_grad():\n","            embeds = self.bert(sentence)['last_hidden_state']\n","        enc, _ = self.lstm(embeds)\n","        enc = self.dropout(enc)\n","        feats = self.linear(enc)\n","        return feats\n","\n","    def forward(self, sentence, tags, mask, is_test=False):\n","        emissions = self._get_features(sentence)\n","        if not is_test:  # Training, return loss\n","            loss = -self.crf.forward(emissions, tags, mask, reduction='mean')\n","            return loss\n","        else:  # Testing, return decoding\n","            decode = self.crf.decode(emissions, mask)\n","            return decode"]},{"cell_type":"markdown","metadata":{"id":"ZZmcG62w1Ap2"},"source":["## Roberta_wwm_BiLSTM_CRF"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1721963003302,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"x_hyVBlWRv7o"},"outputs":[],"source":["class Roberta_wwm_BiLSTM_CRF(nn.Module):\n","    def __init__(self, tag_to_ix, embedding_dim=768, hidden_dim=256):\n","        super(Roberta_wwm_BiLSTM_CRF, self).__init__()\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","        self.hidden_dim = hidden_dim\n","        self.embedding_dim = embedding_dim\n","\n","        # Build the DistilBERT model, including outputting all hidden states\n","        config = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\", output_hidden_states=True)\n","        self.bert = AutoModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext\", config=config)\n","\n","\n","        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim//2,\n","                            num_layers=2, bidirectional=True, batch_first=True)\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.linear = nn.Linear(hidden_dim, self.tagset_size)\n","        self.crf = CRF(self.tagset_size, batch_first=True)\n","\n","    def _get_features(self, sentence):\n","        with torch.no_grad():\n","            embeds = self.bert(sentence)['last_hidden_state']\n","        enc, _ = self.lstm(embeds)\n","        enc = self.dropout(enc)\n","        feats = self.linear(enc)\n","        return feats\n","\n","    def forward(self, sentence, tags, mask, is_test=False):\n","        emissions = self._get_features(sentence)\n","        if not is_test:  # Training, return loss\n","            loss = -self.crf.forward(emissions, tags, mask, reduction='mean')\n","            return loss\n","        else:  # Testing, return decoding\n","            decode = self.crf.decode(emissions, mask)\n","            return decode"]},{"cell_type":"markdown","metadata":{"id":"AEChRkoGMSTn"},"source":["## Bert_LoRA_CRF"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1721963003302,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"X2MiWrXtMVcr"},"outputs":[],"source":["class Bert_LoRA_CRF(nn.Module):\n","    def __init__(self, tag_to_ix, num_layers_to_finetune, r, lora_alpha, lora_dropout, embedding_dim=768, hidden_dim=768):\n","        super(Bert_LoRA_CRF, self).__init__()\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","        self.num_layers_to_finetune = num_layers_to_finetune\n","        self.r = r\n","        self.lora_alpha = lora_alpha\n","        self.lora_dropout = lora_dropout\n","        self.hidden_dim = hidden_dim\n","        self.embedding_dim = embedding_dim\n","\n","\n","        # 创建并加载具有隐藏状态的BERT模型。\n","        self.bert = AutoModel.from_pretrained('bert-base-chinese')\n","        # self.bert = AutoModel.from_pretrained('hfl/chinese-llama-2-lora-7b')\n","\n","        # 微调指定的层。\n","        self.fine_tune_layers()\n","\n","        self.lora_config = LoraConfig(\n","            r = self.r,\n","            lora_alpha = self.lora_alpha,\n","            lora_dropout = self.lora_dropout,\n","            task_type = None,\n","            # task_type = TaskType.SEQ_CLS, # TypeError: forward() got an unexpected keyword argument 'labels'\n","            # task_type = TaskType.CAUSAL_LM, # TypeError: forward() got an unexpected keyword argument 'labels'\n","            # task_type = TaskType.SEQ_2_SEQ_LM, # TypeError: forward() got an unexpected keyword argument 'decoder_input_ids'\n","            # task_type = TaskType.TOKEN_CLS, # TypeError: forward() got an unexpected keyword argument 'labels'\n","            inference_mode = True,\n","        )\n","\n","        self.bert = get_peft_model(self.bert, self.lora_config)\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.linear = nn.Linear(hidden_dim, self.tagset_size)\n","        self.crf = CRF(self.tagset_size, batch_first=True)\n","\n","    def fine_tune_layers(self):\n","        # 冻结所有层。\n","        for param in self.bert.parameters():\n","            param.requires_grad = False\n","\n","        # # 解冻指定的层。\n","        for i in range(12 - self.num_layers_to_finetune, 12):\n","            for param in self.bert.encoder.layer[i].parameters():\n","                param.requires_grad = True\n","\n","    def _get_features(self, sentence,mask):\n","        # with torch.no_grad():\n","        #   embeds = self.bert(sentence)['last_hidden_state']\n","        # enc, _ = self.lstm(embeds)\n","        # enc = self.dropout(enc)\n","        # feats = self.linear(enc)\n","        with torch.no_grad():\n","           embeds = self.bert(sentence, attention_mask=mask)\n","        enc = self.dropout(embeds)\n","        feats = self.linear(enc)\n","        return feats\n","\n","    def forward(self, sentence, tags, mask, is_test=False):\n","        emissions = self._get_features(sentence,mask)\n","        if not is_test: # Training，return loss\n","            loss=-self.crf.forward(emissions, tags, mask, reduction='mean')\n","            return loss\n","        else: # Testing，return decoding\n","            decode=self.crf.decode(emissions, mask)\n","            return decode"]},{"cell_type":"markdown","metadata":{"id":"EJzwnbUWHaMS"},"source":["# Main_config"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["trainset = 'data/train.txt'\n","testset = 'data/test.txt'\n","validset=\"data/msra_eval.txt\""]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["labels = ['O', 'B-BODY','I-BODY', 'B-SYMP', 'I-SYMP', 'B-INST', 'I-INST', 'B-EXAM', 'I-EXAM','B-CHEM', 'I-CHEM',\n","          'B-DISE', 'I-DISE', 'B-DRUG', 'I-DRUG', 'B-SUPP', 'I-SUPP', 'B-TREAT', 'I-TREAT', 'B-TIME', 'I-TIME']"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":1587,"status":"ok","timestamp":1721963004888,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"zcGZ2txDHY4R"},"outputs":[],"source":["import os\n","import time\n","import torch\n","import warnings\n","import numpy as np\n","import torch.nn as nn\n","from sklearn import metrics\n","import torch.optim as optim\n","from torch.utils import data\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","\n","def train(e, model, iterator, optimizer, scheduler, device):\n","    start_time = time.time()  # Record the start time\n","\n","    model.train().to(device)\n","    losses = 0.0\n","    step = 0\n","    for i, batch in enumerate(iterator):\n","        step += 1\n","        x, y, z = batch\n","        x = x.to(device)\n","        y = y.to(device)\n","        z = z.to(device)\n","\n","        loss = model(x, y, z)\n","        losses += loss.item()\n","        \"\"\" Gradient Accumulation \"\"\"\n","        '''\n","          full_loss = loss / 2                            # normalize loss\n","          full_loss.backward()                            # backward and accumulate gradient\n","          if step % 2 == 0:\n","              optimizer.step()                            # update optimizer\n","              scheduler.step()                            # update scheduler\n","              optimizer.zero_grad()                       # clear gradient\n","        '''\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","    end_time = time.time()  # Record the end time\n","    epoch_time = end_time - start_time\n","\n","    print(\"Epoch: {}, Loss:{:.4f}, epoch_time:{:.2f} sec\".format(e, losses/step, epoch_time))\n","\n","def validate(e, model, iterator, device):\n","    start_time = time.time()  # Record the start time\n","\n","    model.eval()\n","    Y, Y_hat = [], []\n","    losses = 0\n","    step = 0\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            step += 1\n","\n","            x, y, z = batch\n","            x = x.to(device)\n","            y = y.to(device)\n","            z = z.to(device)\n","\n","            y_hat = model(x, y, z, is_test=True)\n","\n","            loss = model(x, y, z)\n","            losses += loss.item()\n","            # Save prediction\n","            for j in y_hat:\n","              Y_hat.extend(j)\n","            # Save labels\n","            mask = (z==1)\n","            y_orig = torch.masked_select(y, mask)\n","            Y.append(y_orig.cpu())\n","\n","    Y = torch.cat(Y, dim=0).numpy()\n","    Y_hat = np.array(Y_hat)\n","    acc = (Y_hat == Y).mean()*100\n","\n","    end_time = time.time()  # Record the end time\n","    epoch_time = end_time - start_time\n","\n","    print(\"Epoch: {}, Val Loss:{:.4f}, Val Acc:{:.3f}, epoch_time:{:.2f} sec\".format(e, losses/step, acc, epoch_time))\n","    return model, losses/step, acc\n","\n","def test(model, iterator, device):\n","    model.eval()\n","    Y, Y_hat = [], []\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            x, y, z = batch\n","            x = x.to(device)\n","            z = z.to(device)\n","            y_hat = model(x, y, z, is_test=True)\n","            # Save prediction\n","            for j in y_hat:\n","              Y_hat.extend(j)\n","            # Save labels\n","            mask = (z==1).cpu()\n","            y_orig = torch.masked_select(y, mask)\n","            Y.append(y_orig)\n","\n","    Y = torch.cat(Y, dim=0).numpy()\n","    y_true = [idx2tag[i] for i in Y]\n","    y_pred = [idx2tag[i] for i in Y_hat]\n","\n","    return y_true, y_pred"]},{"cell_type":"markdown","metadata":{"id":"Q8TnpuhCFZq5"},"source":["## Bert + Bi-LSTM + CRF"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269,"referenced_widgets":["e40ad0843de843ddbc730c2f10eb7102","9d5cded00a4049548e7581d81e251217","8658eac06e0845ae9e1b622b263abf83","fed8e6d10edf41b0a6c86764b502204f","90674a218cfe4781a53c16b394ec0866","4c23b05c54034bfcbdeb4494292310f7","50540b5f1b9842af82881690ecdb909b","b024bb1ae9174965b77514d8d78fd2d1","f826b8cca056498a97304065a3da9291","f96d9c8bc3eb4706ac69ae3b68bb7a3b","a4cb44dc8c47446c8dfb2e3aeaba6c6c","b9e1f0c8c9834936be229f74d5360a18","e6cfa3f4ca0144bd8930d9f47ce947f7","04d28448ae684b0c8502b22aeb09585a","586480b6068d4c538d79e3f5eaa48195","e13991a513634a108e4f888bccf35b70","d80c96cb5cf64395aea007d6f81208a2","86ec6e73d0b249e4aa6a022311960b57","8dd450c4c0774066ac9826f523cbd2ad","eee3b475e88d4dbe9061964173a0031f","6e866ade8dd94742955060a48e5d06bc","5fb0958544c541baa3d915a8e8c9845c","cace93073f9b4a5ca3613ca5aed94185","13c221fa0746409090b164acbd92ffb2","a0154f7ff4de4202b8993220887dd180","47d7737f2c2c43c19c39c66c8193cfcf","47008debcf2b415c8847cf1330c0a5df","9496307dd8324d079e9f2934522f1419","3f3d4386bf9f4c3bbdf44fd4ade5541c","56a6f68512d34457995313deadf9aad5","442acb2426594c39ac8a5b31998c3e20","9711353f777247de87f6a92b6b5fdcfc","0dc83c1bb26a49dcb6f6b6d51ea2f804","b9f195c3921046ed92ad375be09c5b29","b3f269af35e84dbda7908824d7d5a9fd","a0700bf315294bc0968783cd8033d5b7","c014a03a74884d13a367bf15735b5606","650e2f253d744d239befb60a4fbf51c8","10f628e7155b412fb8233e85f41af754","0576b107f5c8444eac0ff0bc725d078f","edbe2ea0b351435895b988305d40de89","03d2b26a6eab4bfd895c1c276f24837a","80b00dacfc1640c6b45484422c94a7a2","eefe5a56e14340aabf09bf83e1a9a16e"]},"executionInfo":{"elapsed":2497,"status":"ok","timestamp":1721963007383,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"kGTNz-EvSujr","outputId":"9aabcf83-6e3d-46f9-a3ca-4fb2a61eb188"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from transformers import BertConfig, AutoTokenizer\n","from peft import PeftConfig\n","\n","import os\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:2048\"\n","\n","bert_model = 'bert-base-chinese'\n","\n","tokenizer = AutoTokenizer.from_pretrained(bert_model)\n","\n","VOCAB = ('<PAD>', '[CLS]', '[SEP]', 'O', 'B-BODY', 'I-BODY',\n","         'B-SYMP', 'I-SYMP', 'B-INST', 'I-INST', 'B-EXAM', 'I-EXAM',\n","         'B-CHEM', 'I-CHEM','B-DISE', 'I-DISE', 'B-DRUG', 'I-DRUG',\n","         'B-SUPP', 'I-SUPP', 'B-TREAT', 'I-TREAT', 'B-TIME', 'I-TIME')\n","\n","tag2idx = {tag: idx for idx, tag in enumerate(VOCAB)}\n","idx2tag = {idx: tag for idx, tag in enumerate(VOCAB)}\n","MAX_LEN = 256 - 2\n","\n","bert_config = BertConfig.from_pretrained(bert_model, output_hidden_states=True)\n","\n","class NerDataset(Dataset):\n","    ''' Generate our dataset '''\n","\n","    def __init__(self, f_path):\n","        self.sents = []\n","        self.tags_li = []\n","\n","        with open(f_path, 'r', encoding='utf-8') as f:\n","            lines = [line.split('\\n')[0] for line in f.readlines() if len(line.strip())!=0]\n","\n","        tags =  [line.split('\\t')[1] for line in lines]\n","        words = [line.split('\\t')[0] for line in lines]\n","\n","        word, tag = [], []\n","        for char, t in zip(words, tags):\n","            if char != '。':\n","                word.append(char)\n","                tag.append(t)\n","            else:\n","                if len(word) > MAX_LEN:\n","                  self.sents.append(['[CLS]'] + word[:MAX_LEN] + ['[SEP]'])\n","                  self.tags_li.append(['[CLS]'] + tag[:MAX_LEN] + ['[SEP]'])\n","                else:\n","                  self.sents.append(['[CLS]'] + word + ['[SEP]'])\n","                  self.tags_li.append(['[CLS]'] + tag + ['[SEP]'])\n","                word, tag = [], []\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx]\n","        token_ids = tokenizer.convert_tokens_to_ids(words)\n","        laebl_ids = [tag2idx[tag] for tag in tags]\n","        seqlen = len(laebl_ids)\n","        return token_ids, laebl_ids, seqlen\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","def PadBatch(batch):#[Pad]\n","    maxlen = max([i[2] for i in batch])\n","    token_tensors = torch.LongTensor([i[0] + [0] * (maxlen - len(i[0])) for i in batch])\n","    label_tensors = torch.LongTensor([i[1] + [0] * (maxlen - len(i[1])) for i in batch])\n","    # mask = (token_tensors > 0)\n","    mask = (token_tensors > 0).to(torch.bool)\n","\n","    return token_tensors, label_tensors, mask"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1721963007383,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"ufTXiTWhR8ra"},"outputs":[],"source":["def main(batch_size=64, lr=0.001, n_epochs=40, num_layers_to_finetune=2, r=16, lora_alpha=16,\n","         lora_dropout=0.05, trainset= trainset, validset=validset, testset=testset):\n","\n","    best_model = None\n","    _best_val_loss = 1e18\n","    _best_val_acc = 1e-18\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    model = Bert_BiLSTM_CRF(tag2idx).cuda()\n","\n","    print('Initial model Done.')\n","    train_dataset = NerDataset(trainset)\n","    eval_dataset = NerDataset(validset)\n","    test_dataset = NerDataset(testset)\n","    print('Load Data Done.')\n","\n","    train_iter = data.DataLoader(dataset=train_dataset,\n","                                 batch_size=batch_size,\n","                                 shuffle=True,\n","                                 num_workers=4,\n","                                 collate_fn=PadBatch)\n","\n","    eval_iter = data.DataLoader(dataset=eval_dataset,\n","                                 batch_size=(batch_size)//2,\n","                                 shuffle=False,\n","                                 num_workers=4,\n","                                 collate_fn=PadBatch)\n","\n","    test_iter = data.DataLoader(dataset=test_dataset,\n","                                batch_size=(batch_size)//2,\n","                                shuffle=False,\n","                                num_workers=4,\n","                                collate_fn=PadBatch)\n","\n","    #optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=0.01)\n","    optimizer = AdamW(model.parameters(), lr=lr, eps=1e-6)\n","\n","    # Warmup\n","    len_dataset = len(train_dataset)\n","    epoch = n_epochs\n","    batch_size = batch_size\n","    total_steps = (len_dataset // batch_size) * epoch if len_dataset % batch_size == 0 else (len_dataset // batch_size + 1) * epoch\n","\n","    warm_up_ratio = 0.1 # Define 10% steps\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warm_up_ratio * total_steps, num_training_steps = total_steps)\n","\n","    print('Start Train...,')\n","    for epoch in range(1, n_epochs+1):\n","\n","        train(epoch, model, train_iter, optimizer, scheduler, device)\n","        candidate_model, loss, acc = validate(epoch, model, eval_iter, device)\n","\n","        save_dir = '/home/yenling/Code In Lunix/mamba/Save Model'\n","        os.makedirs(save_dir, exist_ok=True)\n","\n","        if loss < _best_val_loss and acc > _best_val_acc:\n","          best_model = candidate_model\n","          _best_val_loss = loss\n","          _best_val_acc = acc\n","\n","          save_path = os.path.join(save_dir, f'Roberta + WWM+ Bi-LSTM + CRF best_model_epoch_{epoch}_loss_{loss:.4f}_acc_{acc:.4f}.pt')\n","          torch.save(best_model.state_dict(), save_path)\n","          print(f\"Best model saved at epoch {epoch} with val_loss: {loss:.4f} and val_acc: {acc:.4f} to {save_path}\")\n","\n","        print(\"=============================================\")\n","\n","    y_test, y_pred = test(best_model, test_iter, device)\n","    print(metrics.classification_report(y_test, y_pred, labels=labels, digits=3))\n","    print(metrics.confusion_matrix(y_test, y_pred, labels=labels))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336,"referenced_widgets":["87f2bfd10d144b01b98ba684e5688e57","e8541fe952984e5199173371577b92a1","8043f8be4c464240ae0aab7e928fb00a","7275530377f943afb65fadfa9cba3a24","4751dd2f5b684d6b849c20feaa7ea99e","dea6160808e7413d80828da3f298e34f","4f89797d892f4be7aab94463b7d72f8a","77270157015640468486fb862767d41e","acca9d1257e14acfb611e43ebcae8460","bb49f9e17a37477ab5c8827b8dd67b17","3582e4b1d8ca409ca97c7a3d90a19bea"]},"executionInfo":{"elapsed":7284,"status":"error","timestamp":1721963014665,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"ZXtVKyE62PCR","outputId":"a2457446-846a-4e6e-c885-4830bdfba44c"},"outputs":[],"source":["main(n_epochs=100)"]},{"cell_type":"markdown","metadata":{"id":"uvpl95iGGoVg"},"source":["## Bert + CRF"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1721963014666,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"IEojImQtIz6v"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from transformers import BertConfig, AutoTokenizer\n","from peft import PeftConfig\n","\n","import os\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:2048\"\n","\n","bert_model = 'bert-base-chinese'\n","\n","tokenizer = AutoTokenizer.from_pretrained(bert_model)\n","\n","VOCAB = ('<PAD>', '[CLS]', '[SEP]', 'O', 'B-BODY', 'I-BODY',\n","         'B-SYMP', 'I-SYMP', 'B-INST', 'I-INST', 'B-EXAM', 'I-EXAM',\n","         'B-CHEM', 'I-CHEM','B-DISE', 'I-DISE', 'B-DRUG', 'I-DRUG',\n","         'B-SUPP', 'I-SUPP', 'B-TREAT', 'I-TREAT', 'B-TIME', 'I-TIME')\n","\n","tag2idx = {tag: idx for idx, tag in enumerate(VOCAB)}\n","idx2tag = {idx: tag for idx, tag in enumerate(VOCAB)}\n","MAX_LEN = 256 - 2\n","\n","bert_config = BertConfig.from_pretrained(bert_model, output_hidden_states=True)\n","\n","\n","class NerDataset(Dataset):\n","    ''' Generate our dataset '''\n","\n","    def __init__(self, f_path):\n","        self.sents = []\n","        self.tags_li = []\n","\n","        with open(f_path, 'r', encoding='utf-8') as f:\n","            lines = [line.split('\\n')[0] for line in f.readlines() if len(line.strip())!=0]\n","\n","        tags =  [line.split('\\t')[1] for line in lines]\n","        words = [line.split('\\t')[0] for line in lines]\n","\n","        word, tag = [], []\n","        for char, t in zip(words, tags):\n","            if char != '。':\n","                word.append(char)\n","                tag.append(t)\n","            else:\n","                if len(word) > MAX_LEN:\n","                  self.sents.append(['[CLS]'] + word[:MAX_LEN] + ['[SEP]'])\n","                  self.tags_li.append(['[CLS]'] + tag[:MAX_LEN] + ['[SEP]'])\n","                else:\n","                  self.sents.append(['[CLS]'] + word + ['[SEP]'])\n","                  self.tags_li.append(['[CLS]'] + tag + ['[SEP]'])\n","                word, tag = [], []\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx]\n","        token_ids = tokenizer.convert_tokens_to_ids(words)\n","        laebl_ids = [tag2idx[tag] for tag in tags]\n","        seqlen = len(laebl_ids)\n","        return token_ids, laebl_ids, seqlen\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","def PadBatch(batch):#[Pad]\n","    maxlen = max([i[2] for i in batch])\n","    token_tensors = torch.LongTensor([i[0] + [0] * (maxlen - len(i[0])) for i in batch])\n","    label_tensors = torch.LongTensor([i[1] + [0] * (maxlen - len(i[1])) for i in batch])\n","    # mask = (token_tensors > 0)\n","    mask = (token_tensors > 0).to(torch.bool)\n","\n","    return token_tensors, label_tensors, mask"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1721963014666,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"FIVTylVoIAnz"},"outputs":[],"source":["def main(batch_size=64, lr=0.001, n_epochs=40, num_layers_to_finetune=2, r=16, lora_alpha=16,\n","         lora_dropout=0.05, trainset= trainset, validset=validset, testset=testset):\n","\n","    best_model = None\n","    _best_val_loss = 1e18\n","    _best_val_acc = 1e-18\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","\n","    # model = Bert_CRF(tag2idx,num_layers_to_finetune).cuda()\n","    model = Bert_CRF(tag2idx).cuda()\n","\n","\n","    print('Initial model Done.')\n","    train_dataset = NerDataset(trainset)\n","    eval_dataset = NerDataset(validset)\n","    test_dataset = NerDataset(testset)\n","    print('Load Data Done.')\n","\n","    train_iter = data.DataLoader(dataset=train_dataset,\n","                                 batch_size=batch_size,\n","                                 shuffle=True,\n","                                 num_workers=4,\n","                                 collate_fn=PadBatch)\n","\n","    eval_iter = data.DataLoader(dataset=eval_dataset,\n","                                 batch_size=(batch_size)//2,\n","                                 shuffle=False,\n","                                 num_workers=4,\n","                                 collate_fn=PadBatch)\n","\n","    test_iter = data.DataLoader(dataset=test_dataset,\n","                                batch_size=(batch_size)//2,\n","                                shuffle=False,\n","                                num_workers=4,\n","                                collate_fn=PadBatch)\n","\n","    #optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=0.01)\n","    optimizer = AdamW(model.parameters(), lr=lr, eps=1e-6)\n","\n","    # Warmup\n","    len_dataset = len(train_dataset)\n","    epoch = n_epochs\n","    batch_size = batch_size\n","    total_steps = (len_dataset // batch_size) * epoch if len_dataset % batch_size == 0 else (len_dataset // batch_size + 1) * epoch\n","\n","    warm_up_ratio = 0.1 # Define 10% steps\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warm_up_ratio * total_steps, num_training_steps = total_steps)\n","\n","    print('Start Train...,')\n","    for epoch in range(1, n_epochs+1):\n","\n","        train(epoch, model, train_iter, optimizer, scheduler, device)\n","        candidate_model, loss, acc = validate(epoch, model, eval_iter, device)\n","\n","        save_dir = '/home/yenling/Code In Lunix/mamba/Save Model'\n","        os.makedirs(save_dir, exist_ok=True)\n","\n","        if loss < _best_val_loss and acc > _best_val_acc:\n","          best_model = candidate_model\n","          _best_val_loss = loss\n","          _best_val_acc = acc\n","\n","          save_path = os.path.join(save_dir, f'Roberta + WWM+ Bi-LSTM + CRF best_model_epoch_{epoch}_loss_{loss:.4f}_acc_{acc:.4f}.pt')\n","          torch.save(best_model.state_dict(), save_path)\n","          print(f\"Best model saved at epoch {epoch} with val_loss: {loss:.4f} and val_acc: {acc:.4f} to {save_path}\")\n","\n","        print(\"=============================================\")\n","\n","    y_test, y_pred = test(best_model, test_iter, device)\n","    print(metrics.classification_report(y_test, y_pred, labels=labels, digits=3))\n","    print(metrics.confusion_matrix(y_test, y_pred, labels=labels))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1721963014666,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"v_msKXYqG5Rv"},"outputs":[],"source":["main(n_epochs=100)"]},{"cell_type":"markdown","metadata":{"id":"gqkoGz8BGq_k"},"source":["## ClinicalDistil BERT + Bi-LSTM + CRF\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1721963014667,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"Rv_iQegaI5Fv"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from transformers import BertConfig, AutoTokenizer\n","from peft import PeftConfig\n","\n","import os\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:2048\"\n","\n","bert_model = 'medicalai/ClinicalBERT'\n","\n","tokenizer = AutoTokenizer.from_pretrained(bert_model)\n","\n","VOCAB = ('<PAD>', '[CLS]', '[SEP]', 'O', 'B-BODY', 'I-BODY',\n","         'B-SYMP', 'I-SYMP', 'B-INST', 'I-INST', 'B-EXAM', 'I-EXAM',\n","         'B-CHEM', 'I-CHEM','B-DISE', 'I-DISE', 'B-DRUG', 'I-DRUG',\n","         'B-SUPP', 'I-SUPP', 'B-TREAT', 'I-TREAT', 'B-TIME', 'I-TIME')\n","\n","tag2idx = {tag: idx for idx, tag in enumerate(VOCAB)}\n","idx2tag = {idx: tag for idx, tag in enumerate(VOCAB)}\n","MAX_LEN = 256 - 2\n","\n","bert_config = BertConfig.from_pretrained(bert_model, output_hidden_states=True)\n","\n","\n","class NerDataset(Dataset):\n","    ''' Generate our dataset '''\n","\n","    def __init__(self, f_path):\n","        self.sents = []\n","        self.tags_li = []\n","\n","        with open(f_path, 'r', encoding='utf-8') as f:\n","            lines = [line.split('\\n')[0] for line in f.readlines() if len(line.strip())!=0]\n","\n","        tags =  [line.split('\\t')[1] for line in lines]\n","        words = [line.split('\\t')[0] for line in lines]\n","\n","        word, tag = [], []\n","        for char, t in zip(words, tags):\n","            if char != '。':\n","                word.append(char)\n","                tag.append(t)\n","            else:\n","                if len(word) > MAX_LEN:\n","                  self.sents.append(['[CLS]'] + word[:MAX_LEN] + ['[SEP]'])\n","                  self.tags_li.append(['[CLS]'] + tag[:MAX_LEN] + ['[SEP]'])\n","                else:\n","                  self.sents.append(['[CLS]'] + word + ['[SEP]'])\n","                  self.tags_li.append(['[CLS]'] + tag + ['[SEP]'])\n","                word, tag = [], []\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx]\n","        token_ids = tokenizer.convert_tokens_to_ids(words)\n","        laebl_ids = [tag2idx[tag] for tag in tags]\n","        seqlen = len(laebl_ids)\n","        return token_ids, laebl_ids, seqlen\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","def PadBatch(batch):#[Pad]\n","    maxlen = max([i[2] for i in batch])\n","    token_tensors = torch.LongTensor([i[0] + [0] * (maxlen - len(i[0])) for i in batch])\n","    label_tensors = torch.LongTensor([i[1] + [0] * (maxlen - len(i[1])) for i in batch])\n","    # mask = (token_tensors > 0)\n","    mask = (token_tensors > 0).to(torch.bool)\n","\n","    return token_tensors, label_tensors, mask"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1721963014667,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"s7xX_1kEH8hd"},"outputs":[],"source":["def main(batch_size=64, lr=0.001, n_epochs=40, num_layers_to_finetune=2, r=16, lora_alpha=16,\n","         lora_dropout=0.05, trainset= trainset, validset=validset, testset=testset):\n","\n","    best_model = None\n","    _best_val_loss = 1e18\n","    _best_val_acc = 1e-18\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    model = ClinicalDistilBERT_BiLSTM_CRF(tag2idx).cuda()\n","\n","    print('Initial model Done.')\n","    train_dataset = NerDataset(trainset)\n","    eval_dataset = NerDataset(validset)\n","    test_dataset = NerDataset(testset)\n","    print('Load Data Done.')\n","\n","    train_iter = data.DataLoader(dataset=train_dataset,\n","                                 batch_size=batch_size,\n","                                 shuffle=True,\n","                                 num_workers=4,\n","                                 collate_fn=PadBatch)\n","\n","    eval_iter = data.DataLoader(dataset=eval_dataset,\n","                                 batch_size=(batch_size)//2,\n","                                 shuffle=False,\n","                                 num_workers=4,\n","                                 collate_fn=PadBatch)\n","\n","    test_iter = data.DataLoader(dataset=test_dataset,\n","                                batch_size=(batch_size)//2,\n","                                shuffle=False,\n","                                num_workers=4,\n","                                collate_fn=PadBatch)\n","\n","    #optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=0.01)\n","    optimizer = AdamW(model.parameters(), lr=lr, eps=1e-6)\n","\n","    # Warmup\n","    len_dataset = len(train_dataset)\n","    epoch = n_epochs\n","    batch_size = batch_size\n","    total_steps = (len_dataset // batch_size) * epoch if len_dataset % batch_size == 0 else (len_dataset // batch_size + 1) * epoch\n","\n","    warm_up_ratio = 0.1 # Define 10% steps\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warm_up_ratio * total_steps, num_training_steps = total_steps)\n","\n","    print('Start Train...,')\n","    for epoch in range(1, n_epochs+1):\n","\n","        train(epoch, model, train_iter, optimizer, scheduler, device)\n","        candidate_model, loss, acc = validate(epoch, model, eval_iter, device)\n","\n","        save_dir = '/home/yenling/Code In Lunix/mamba/Save Model'\n","        os.makedirs(save_dir, exist_ok=True)\n","\n","        if loss < _best_val_loss and acc > _best_val_acc:\n","          best_model = candidate_model\n","          _best_val_loss = loss\n","          _best_val_acc = acc\n","\n","          save_path = os.path.join(save_dir, f'Roberta + WWM+ Bi-LSTM + CRF best_model_epoch_{epoch}_loss_{loss:.4f}_acc_{acc:.4f}.pt')\n","          torch.save(best_model.state_dict(), save_path)\n","          print(f\"Best model saved at epoch {epoch} with val_loss: {loss:.4f} and val_acc: {acc:.4f} to {save_path}\")\n","\n","        print(\"=============================================\")\n","\n","    y_test, y_pred = test(best_model, test_iter, device)\n","    print(metrics.classification_report(y_test, y_pred, labels=labels, digits=3))\n","    print(metrics.confusion_matrix(y_test, y_pred, labels=labels))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1721963014667,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"RgiqG2eRG5xW"},"outputs":[],"source":["main(n_epochs=100)"]},{"cell_type":"markdown","metadata":{"id":"hrDyKHuaGuwv"},"source":["## Mac BERT + Bi-LSTM + CRF\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1721963014667,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"vqKarLtEI-zh"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from transformers import BertConfig, AutoTokenizer\n","from peft import PeftConfig\n","\n","import os\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:2048\"\n","\n","bert_model = 'hfl/chinese-macbert-base'\n","\n","tokenizer = AutoTokenizer.from_pretrained(bert_model)\n","\n","VOCAB = ('<PAD>', '[CLS]', '[SEP]', 'O', 'B-BODY', 'I-BODY',\n","         'B-SYMP', 'I-SYMP', 'B-INST', 'I-INST', 'B-EXAM', 'I-EXAM',\n","         'B-CHEM', 'I-CHEM','B-DISE', 'I-DISE', 'B-DRUG', 'I-DRUG',\n","         'B-SUPP', 'I-SUPP', 'B-TREAT', 'I-TREAT', 'B-TIME', 'I-TIME')\n","\n","tag2idx = {tag: idx for idx, tag in enumerate(VOCAB)}\n","idx2tag = {idx: tag for idx, tag in enumerate(VOCAB)}\n","MAX_LEN = 256 - 2\n","\n","bert_config = BertConfig.from_pretrained(bert_model, output_hidden_states=True)\n","\n","\n","class NerDataset(Dataset):\n","    ''' Generate our dataset '''\n","\n","    def __init__(self, f_path):\n","        self.sents = []\n","        self.tags_li = []\n","\n","        with open(f_path, 'r', encoding='utf-8') as f:\n","            lines = [line.split('\\n')[0] for line in f.readlines() if len(line.strip())!=0]\n","\n","        tags =  [line.split('\\t')[1] for line in lines]\n","        words = [line.split('\\t')[0] for line in lines]\n","\n","        word, tag = [], []\n","        for char, t in zip(words, tags):\n","            if char != '。':\n","                word.append(char)\n","                tag.append(t)\n","            else:\n","                if len(word) > MAX_LEN:\n","                  self.sents.append(['[CLS]'] + word[:MAX_LEN] + ['[SEP]'])\n","                  self.tags_li.append(['[CLS]'] + tag[:MAX_LEN] + ['[SEP]'])\n","                else:\n","                  self.sents.append(['[CLS]'] + word + ['[SEP]'])\n","                  self.tags_li.append(['[CLS]'] + tag + ['[SEP]'])\n","                word, tag = [], []\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx]\n","        token_ids = tokenizer.convert_tokens_to_ids(words)\n","        laebl_ids = [tag2idx[tag] for tag in tags]\n","        seqlen = len(laebl_ids)\n","        return token_ids, laebl_ids, seqlen\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","def PadBatch(batch):#[Pad]\n","    maxlen = max([i[2] for i in batch])\n","    token_tensors = torch.LongTensor([i[0] + [0] * (maxlen - len(i[0])) for i in batch])\n","    label_tensors = torch.LongTensor([i[1] + [0] * (maxlen - len(i[1])) for i in batch])\n","    # mask = (token_tensors > 0)\n","    mask = (token_tensors > 0).to(torch.bool)\n","\n","    return token_tensors, label_tensors, mask"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1721963014667,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"CKMWH5MqH0l8"},"outputs":[],"source":["def main(batch_size=64, lr=0.001, n_epochs=40, num_layers_to_finetune=2, r=16, lora_alpha=16,\n","         lora_dropout=0.05, trainset= trainset, validset=validset, testset=testset):\n","\n","    best_model = None\n","    _best_val_loss = 1e18\n","    _best_val_acc = 1e-18\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","\n","    model = MacBERT_BiLSTM_CRF(tag2idx).cuda()\n","\n","    print('Initial model Done.')\n","    train_dataset = NerDataset(trainset)\n","    eval_dataset = NerDataset(validset)\n","    test_dataset = NerDataset(testset)\n","    print('Load Data Done.')\n","\n","    train_iter = data.DataLoader(dataset=train_dataset,\n","                                 batch_size=batch_size,\n","                                 shuffle=True,\n","                                 num_workers=4,\n","                                 collate_fn=PadBatch)\n","\n","    eval_iter = data.DataLoader(dataset=eval_dataset,\n","                                 batch_size=(batch_size)//2,\n","                                 shuffle=False,\n","                                 num_workers=4,\n","                                 collate_fn=PadBatch)\n","\n","    test_iter = data.DataLoader(dataset=test_dataset,\n","                                batch_size=(batch_size)//2,\n","                                shuffle=False,\n","                                num_workers=4,\n","                                collate_fn=PadBatch)\n","\n","    #optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=0.01)\n","    optimizer = AdamW(model.parameters(), lr=lr, eps=1e-6)\n","\n","    # Warmup\n","    len_dataset = len(train_dataset)\n","    epoch = n_epochs\n","    batch_size = batch_size\n","    total_steps = (len_dataset // batch_size) * epoch if len_dataset % batch_size == 0 else (len_dataset // batch_size + 1) * epoch\n","\n","    warm_up_ratio = 0.1 # Define 10% steps\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warm_up_ratio * total_steps, num_training_steps = total_steps)\n","\n","    print('Start Train...,')\n","    for epoch in range(1, n_epochs+1):\n","\n","        train(epoch, model, train_iter, optimizer, scheduler, device)\n","        candidate_model, loss, acc = validate(epoch, model, eval_iter, device)\n","\n","        save_dir = '/home/yenling/Code In Lunix/mamba/Save Model'\n","        os.makedirs(save_dir, exist_ok=True)\n","\n","        if loss < _best_val_loss and acc > _best_val_acc:\n","          best_model = candidate_model\n","          _best_val_loss = loss\n","          _best_val_acc = acc\n","\n","          save_path = os.path.join(save_dir, f'Roberta + WWM+ Bi-LSTM + CRF best_model_epoch_{epoch}_loss_{loss:.4f}_acc_{acc:.4f}.pt')\n","          torch.save(best_model.state_dict(), save_path)\n","          print(f\"Best model saved at epoch {epoch} with val_loss: {loss:.4f} and val_acc: {acc:.4f} to {save_path}\")\n","\n","        print(\"=============================================\")\n","\n","    y_test, y_pred = test(best_model, test_iter, device)\n","    print(metrics.classification_report(y_test, y_pred, labels=labels, digits=3))\n","    print(metrics.confusion_matrix(y_test, y_pred, labels=labels))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1721963014667,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"8_5BqP5gG6Te"},"outputs":[],"source":["main(n_epochs=100)"]},{"cell_type":"markdown","metadata":{"id":"0EEeR3BkGyuI"},"source":["## BERTWWM + Bi-LSTM + CRF\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1721963014667,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"DpDadZSVJFR4"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from transformers import BertConfig, AutoTokenizer\n","from peft import PeftConfig\n","\n","import os\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:2048\"\n","\n","\n","bert_model = 'hfl/chinese-bert-wwm'\n","\n","tokenizer = AutoTokenizer.from_pretrained(bert_model)\n","\n","VOCAB = ('<PAD>', '[CLS]', '[SEP]', 'O', 'B-BODY', 'I-BODY',\n","         'B-SYMP', 'I-SYMP', 'B-INST', 'I-INST', 'B-EXAM', 'I-EXAM',\n","         'B-CHEM', 'I-CHEM','B-DISE', 'I-DISE', 'B-DRUG', 'I-DRUG',\n","         'B-SUPP', 'I-SUPP', 'B-TREAT', 'I-TREAT', 'B-TIME', 'I-TIME')\n","\n","tag2idx = {tag: idx for idx, tag in enumerate(VOCAB)}\n","idx2tag = {idx: tag for idx, tag in enumerate(VOCAB)}\n","MAX_LEN = 256 - 2\n","\n","bert_config = BertConfig.from_pretrained(bert_model, output_hidden_states=True)\n","\n","\n","class NerDataset(Dataset):\n","    ''' Generate our dataset '''\n","\n","    def __init__(self, f_path):\n","        self.sents = []\n","        self.tags_li = []\n","\n","        with open(f_path, 'r', encoding='utf-8') as f:\n","            lines = [line.split('\\n')[0] for line in f.readlines() if len(line.strip())!=0]\n","\n","        tags =  [line.split('\\t')[1] for line in lines]\n","        words = [line.split('\\t')[0] for line in lines]\n","\n","        word, tag = [], []\n","        for char, t in zip(words, tags):\n","            if char != '。':\n","                word.append(char)\n","                tag.append(t)\n","            else:\n","                if len(word) > MAX_LEN:\n","                  self.sents.append(['[CLS]'] + word[:MAX_LEN] + ['[SEP]'])\n","                  self.tags_li.append(['[CLS]'] + tag[:MAX_LEN] + ['[SEP]'])\n","                else:\n","                  self.sents.append(['[CLS]'] + word + ['[SEP]'])\n","                  self.tags_li.append(['[CLS]'] + tag + ['[SEP]'])\n","                word, tag = [], []\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx]\n","        token_ids = tokenizer.convert_tokens_to_ids(words)\n","        laebl_ids = [tag2idx[tag] for tag in tags]\n","        seqlen = len(laebl_ids)\n","        return token_ids, laebl_ids, seqlen\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","def PadBatch(batch):#[Pad]\n","    maxlen = max([i[2] for i in batch])\n","    token_tensors = torch.LongTensor([i[0] + [0] * (maxlen - len(i[0])) for i in batch])\n","    label_tensors = torch.LongTensor([i[1] + [0] * (maxlen - len(i[1])) for i in batch])\n","    # mask = (token_tensors > 0)\n","    mask = (token_tensors > 0).to(torch.bool)\n","\n","    return token_tensors, label_tensors, mask"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1721963014667,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"rKYpOSCLHI2Z"},"outputs":[],"source":["def main(batch_size=64, lr=0.001, n_epochs=40, num_layers_to_finetune=2, r=16, lora_alpha=16,\n","         lora_dropout=0.05, trainset= trainset, validset=validset, testset=testset):\n","\n","    best_model = None\n","    _best_val_loss = 1e18\n","    _best_val_acc = 1e-18\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    model = BERTWWM_BiLSTM_CRF(tag2idx).cuda()\n","\n","\n","    print('Initial model Done.')\n","    train_dataset = NerDataset(trainset)\n","    eval_dataset = NerDataset(validset)\n","    test_dataset = NerDataset(testset)\n","    print('Load Data Done.')\n","\n","    train_iter = data.DataLoader(dataset=train_dataset,\n","                                 batch_size=batch_size,\n","                                 shuffle=True,\n","                                 num_workers=4,\n","                                 collate_fn=PadBatch)\n","\n","    eval_iter = data.DataLoader(dataset=eval_dataset,\n","                                 batch_size=(batch_size)//2,\n","                                 shuffle=False,\n","                                 num_workers=4,\n","                                 collate_fn=PadBatch)\n","\n","    test_iter = data.DataLoader(dataset=test_dataset,\n","                                batch_size=(batch_size)//2,\n","                                shuffle=False,\n","                                num_workers=4,\n","                                collate_fn=PadBatch)\n","\n","    #optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=0.01)\n","    optimizer = AdamW(model.parameters(), lr=lr, eps=1e-6)\n","\n","    # Warmup\n","    len_dataset = len(train_dataset)\n","    epoch = n_epochs\n","    batch_size = batch_size\n","    total_steps = (len_dataset // batch_size) * epoch if len_dataset % batch_size == 0 else (len_dataset // batch_size + 1) * epoch\n","\n","    warm_up_ratio = 0.1 # Define 10% steps\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warm_up_ratio * total_steps, num_training_steps = total_steps)\n","\n","    print('Start Train...,')\n","    for epoch in range(1, n_epochs+1):\n","\n","        train(epoch, model, train_iter, optimizer, scheduler, device)\n","        candidate_model, loss, acc = validate(epoch, model, eval_iter, device)\n","\n","        save_dir = '/home/yenling/Code In Lunix/mamba/Save Model'\n","        os.makedirs(save_dir, exist_ok=True)\n","\n","        if loss < _best_val_loss and acc > _best_val_acc:\n","          best_model = candidate_model\n","          _best_val_loss = loss\n","          _best_val_acc = acc\n","\n","          save_path = os.path.join(save_dir, f'Roberta + WWM+ Bi-LSTM + CRF best_model_epoch_{epoch}_loss_{loss:.4f}_acc_{acc:.4f}.pt')\n","          torch.save(best_model.state_dict(), save_path)\n","          print(f\"Best model saved at epoch {epoch} with val_loss: {loss:.4f} and val_acc: {acc:.4f} to {save_path}\")\n","\n","        print(\"=============================================\")\n","\n","    y_test, y_pred = test(best_model, test_iter, device)\n","    print(metrics.classification_report(y_test, y_pred, labels=labels, digits=3))\n","    print(metrics.confusion_matrix(y_test, y_pred, labels=labels))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1721963014667,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"EiaNLzuQG6uJ"},"outputs":[],"source":["main(n_epochs=100)"]},{"cell_type":"markdown","metadata":{"id":"A6RB8t6NG1nL"},"source":["## Roberta + WWM+ Bi-LSTM + CRF\n"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1721963014667,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"5VpFgRgZJLuc"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from transformers import BertConfig, AutoTokenizer\n","from peft import PeftConfig\n","\n","import os\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:2048\"\n","\n","\n","bert_model = 'hfl/chinese-roberta-wwm-ext'\n","\n","tokenizer = AutoTokenizer.from_pretrained(bert_model)\n","\n","VOCAB = ('<PAD>', '[CLS]', '[SEP]', 'O', 'B-BODY', 'I-BODY',\n","         'B-SYMP', 'I-SYMP', 'B-INST', 'I-INST', 'B-EXAM', 'I-EXAM',\n","         'B-CHEM', 'I-CHEM','B-DISE', 'I-DISE', 'B-DRUG', 'I-DRUG',\n","         'B-SUPP', 'I-SUPP', 'B-TREAT', 'I-TREAT', 'B-TIME', 'I-TIME')\n","\n","tag2idx = {tag: idx for idx, tag in enumerate(VOCAB)}\n","idx2tag = {idx: tag for idx, tag in enumerate(VOCAB)}\n","MAX_LEN = 256 - 2\n","\n","bert_config = BertConfig.from_pretrained(bert_model, output_hidden_states=True)\n","\n","\n","class NerDataset(Dataset):\n","    ''' Generate our dataset '''\n","\n","    def __init__(self, f_path):\n","        self.sents = []\n","        self.tags_li = []\n","\n","        with open(f_path, 'r', encoding='utf-8') as f:\n","            lines = [line.split('\\n')[0] for line in f.readlines() if len(line.strip())!=0]\n","\n","        tags =  [line.split('\\t')[1] for line in lines]\n","        words = [line.split('\\t')[0] for line in lines]\n","\n","        word, tag = [], []\n","        for char, t in zip(words, tags):\n","            if char != '。':\n","                word.append(char)\n","                tag.append(t)\n","            else:\n","                if len(word) > MAX_LEN:\n","                  self.sents.append(['[CLS]'] + word[:MAX_LEN] + ['[SEP]'])\n","                  self.tags_li.append(['[CLS]'] + tag[:MAX_LEN] + ['[SEP]'])\n","                else:\n","                  self.sents.append(['[CLS]'] + word + ['[SEP]'])\n","                  self.tags_li.append(['[CLS]'] + tag + ['[SEP]'])\n","                word, tag = [], []\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx]\n","        token_ids = tokenizer.convert_tokens_to_ids(words)\n","        laebl_ids = [tag2idx[tag] for tag in tags]\n","        seqlen = len(laebl_ids)\n","        return token_ids, laebl_ids, seqlen\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","def PadBatch(batch):#[Pad]\n","    maxlen = max([i[2] for i in batch])\n","    token_tensors = torch.LongTensor([i[0] + [0] * (maxlen - len(i[0])) for i in batch])\n","    label_tensors = torch.LongTensor([i[1] + [0] * (maxlen - len(i[1])) for i in batch])\n","    # mask = (token_tensors > 0)\n","    mask = (token_tensors > 0).to(torch.bool)\n","\n","    return token_tensors, label_tensors, mask"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1721963014667,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"5asJzYj7G-Pb"},"outputs":[],"source":["def main(batch_size=64, lr=0.001, n_epochs=40, num_layers_to_finetune=2, r=16, lora_alpha=16,\n","         lora_dropout=0.05, trainset= trainset, validset=validset, testset=testset):\n","\n","    best_model = None\n","    _best_val_loss = 1e18\n","    _best_val_acc = 1e-18\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    model = Roberta_wwm_BiLSTM_CRF(tag2idx).cuda()\n","\n","\n","    print('Initial model Done.')\n","    train_dataset = NerDataset(trainset)\n","    eval_dataset = NerDataset(validset)\n","    test_dataset = NerDataset(testset)\n","    print('Load Data Done.')\n","\n","    train_iter = data.DataLoader(dataset=train_dataset,\n","                                 batch_size=batch_size,\n","                                 shuffle=True,\n","                                 num_workers=4,\n","                                 collate_fn=PadBatch)\n","\n","    eval_iter = data.DataLoader(dataset=eval_dataset,\n","                                 batch_size=(batch_size)//2,\n","                                 shuffle=False,\n","                                 num_workers=4,\n","                                 collate_fn=PadBatch)\n","\n","    test_iter = data.DataLoader(dataset=test_dataset,\n","                                batch_size=(batch_size)//2,\n","                                shuffle=False,\n","                                num_workers=4,\n","                                collate_fn=PadBatch)\n","\n","    #optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=0.01)\n","    optimizer = AdamW(model.parameters(), lr=lr, eps=1e-6)\n","\n","    # Warmup\n","    len_dataset = len(train_dataset)\n","    epoch = n_epochs\n","    batch_size = batch_size\n","    total_steps = (len_dataset // batch_size) * epoch if len_dataset % batch_size == 0 else (len_dataset // batch_size + 1) * epoch\n","\n","    warm_up_ratio = 0.1 # Define 10% steps\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warm_up_ratio * total_steps, num_training_steps = total_steps)\n","\n","    print('Start Train...,')\n","    for epoch in range(1, n_epochs+1):\n","\n","        train(epoch, model, train_iter, optimizer, scheduler, device)\n","        candidate_model, loss, acc = validate(epoch, model, eval_iter, device)\n","\n","        save_dir = '/home/yenling/Code In Lunix/mamba/Save Model'\n","        os.makedirs(save_dir, exist_ok=True)\n","\n","        if loss < _best_val_loss and acc > _best_val_acc:\n","          best_model = candidate_model\n","          _best_val_loss = loss\n","          _best_val_acc = acc\n","\n","          save_path = os.path.join(save_dir, f'Roberta + WWM+ Bi-LSTM + CRF best_model_epoch_{epoch}_loss_{loss:.4f}_acc_{acc:.4f}.pt')\n","          torch.save(best_model.state_dict(), save_path)\n","          print(f\"Best model saved at epoch {epoch} with val_loss: {loss:.4f} and val_acc: {acc:.4f} to {save_path}\")\n","\n","        print(\"=============================================\")\n","\n","    y_test, y_pred = test(best_model, test_iter, device)\n","    print(metrics.classification_report(y_test, y_pred, labels=labels, digits=3))\n","    print(metrics.confusion_matrix(y_test, y_pred, labels=labels))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1721963014667,"user":{"displayName":"黃鈺傑","userId":"13277647934783630609"},"user_tz":-480},"id":"RVP4qFDoG7OB"},"outputs":[],"source":["main(n_epochs=100)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"03d2b26a6eab4bfd895c1c276f24837a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"04d28448ae684b0c8502b22aeb09585a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dd450c4c0774066ac9826f523cbd2ad","max":624,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eee3b475e88d4dbe9061964173a0031f","value":624}},"0576b107f5c8444eac0ff0bc725d078f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0dc83c1bb26a49dcb6f6b6d51ea2f804":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10f628e7155b412fb8233e85f41af754":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13c221fa0746409090b164acbd92ffb2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9496307dd8324d079e9f2934522f1419","placeholder":"​","style":"IPY_MODEL_3f3d4386bf9f4c3bbdf44fd4ade5541c","value":"vocab.txt: 100%"}},"3582e4b1d8ca409ca97c7a3d90a19bea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f3d4386bf9f4c3bbdf44fd4ade5541c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"442acb2426594c39ac8a5b31998c3e20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"47008debcf2b415c8847cf1330c0a5df":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4751dd2f5b684d6b849c20feaa7ea99e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47d7737f2c2c43c19c39c66c8193cfcf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9711353f777247de87f6a92b6b5fdcfc","placeholder":"​","style":"IPY_MODEL_0dc83c1bb26a49dcb6f6b6d51ea2f804","value":" 110k/110k [00:00&lt;00:00, 705kB/s]"}},"4c23b05c54034bfcbdeb4494292310f7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f89797d892f4be7aab94463b7d72f8a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50540b5f1b9842af82881690ecdb909b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56a6f68512d34457995313deadf9aad5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"586480b6068d4c538d79e3f5eaa48195":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e866ade8dd94742955060a48e5d06bc","placeholder":"​","style":"IPY_MODEL_5fb0958544c541baa3d915a8e8c9845c","value":" 624/624 [00:00&lt;00:00, 11.3kB/s]"}},"5fb0958544c541baa3d915a8e8c9845c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"650e2f253d744d239befb60a4fbf51c8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e866ade8dd94742955060a48e5d06bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7275530377f943afb65fadfa9cba3a24":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb49f9e17a37477ab5c8827b8dd67b17","placeholder":"​","style":"IPY_MODEL_3582e4b1d8ca409ca97c7a3d90a19bea","value":" 412M/412M [00:05&lt;00:00, 76.3MB/s]"}},"77270157015640468486fb862767d41e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8043f8be4c464240ae0aab7e928fb00a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_77270157015640468486fb862767d41e","max":411553788,"min":0,"orientation":"horizontal","style":"IPY_MODEL_acca9d1257e14acfb611e43ebcae8460","value":411553788}},"80b00dacfc1640c6b45484422c94a7a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8658eac06e0845ae9e1b622b263abf83":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b024bb1ae9174965b77514d8d78fd2d1","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f826b8cca056498a97304065a3da9291","value":49}},"86ec6e73d0b249e4aa6a022311960b57":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87f2bfd10d144b01b98ba684e5688e57":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e8541fe952984e5199173371577b92a1","IPY_MODEL_8043f8be4c464240ae0aab7e928fb00a","IPY_MODEL_7275530377f943afb65fadfa9cba3a24"],"layout":"IPY_MODEL_4751dd2f5b684d6b849c20feaa7ea99e"}},"8dd450c4c0774066ac9826f523cbd2ad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90674a218cfe4781a53c16b394ec0866":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9496307dd8324d079e9f2934522f1419":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9711353f777247de87f6a92b6b5fdcfc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d5cded00a4049548e7581d81e251217":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c23b05c54034bfcbdeb4494292310f7","placeholder":"​","style":"IPY_MODEL_50540b5f1b9842af82881690ecdb909b","value":"tokenizer_config.json: 100%"}},"a0154f7ff4de4202b8993220887dd180":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_56a6f68512d34457995313deadf9aad5","max":109540,"min":0,"orientation":"horizontal","style":"IPY_MODEL_442acb2426594c39ac8a5b31998c3e20","value":109540}},"a0700bf315294bc0968783cd8033d5b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_edbe2ea0b351435895b988305d40de89","max":268943,"min":0,"orientation":"horizontal","style":"IPY_MODEL_03d2b26a6eab4bfd895c1c276f24837a","value":268943}},"a4cb44dc8c47446c8dfb2e3aeaba6c6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"acca9d1257e14acfb611e43ebcae8460":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b024bb1ae9174965b77514d8d78fd2d1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3f269af35e84dbda7908824d7d5a9fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10f628e7155b412fb8233e85f41af754","placeholder":"​","style":"IPY_MODEL_0576b107f5c8444eac0ff0bc725d078f","value":"tokenizer.json: 100%"}},"b9e1f0c8c9834936be229f74d5360a18":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6cfa3f4ca0144bd8930d9f47ce947f7","IPY_MODEL_04d28448ae684b0c8502b22aeb09585a","IPY_MODEL_586480b6068d4c538d79e3f5eaa48195"],"layout":"IPY_MODEL_e13991a513634a108e4f888bccf35b70"}},"b9f195c3921046ed92ad375be09c5b29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b3f269af35e84dbda7908824d7d5a9fd","IPY_MODEL_a0700bf315294bc0968783cd8033d5b7","IPY_MODEL_c014a03a74884d13a367bf15735b5606"],"layout":"IPY_MODEL_650e2f253d744d239befb60a4fbf51c8"}},"bb49f9e17a37477ab5c8827b8dd67b17":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c014a03a74884d13a367bf15735b5606":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80b00dacfc1640c6b45484422c94a7a2","placeholder":"​","style":"IPY_MODEL_eefe5a56e14340aabf09bf83e1a9a16e","value":" 269k/269k [00:00&lt;00:00, 4.98MB/s]"}},"cace93073f9b4a5ca3613ca5aed94185":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_13c221fa0746409090b164acbd92ffb2","IPY_MODEL_a0154f7ff4de4202b8993220887dd180","IPY_MODEL_47d7737f2c2c43c19c39c66c8193cfcf"],"layout":"IPY_MODEL_47008debcf2b415c8847cf1330c0a5df"}},"d80c96cb5cf64395aea007d6f81208a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dea6160808e7413d80828da3f298e34f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e13991a513634a108e4f888bccf35b70":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e40ad0843de843ddbc730c2f10eb7102":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d5cded00a4049548e7581d81e251217","IPY_MODEL_8658eac06e0845ae9e1b622b263abf83","IPY_MODEL_fed8e6d10edf41b0a6c86764b502204f"],"layout":"IPY_MODEL_90674a218cfe4781a53c16b394ec0866"}},"e6cfa3f4ca0144bd8930d9f47ce947f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d80c96cb5cf64395aea007d6f81208a2","placeholder":"​","style":"IPY_MODEL_86ec6e73d0b249e4aa6a022311960b57","value":"config.json: 100%"}},"e8541fe952984e5199173371577b92a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dea6160808e7413d80828da3f298e34f","placeholder":"​","style":"IPY_MODEL_4f89797d892f4be7aab94463b7d72f8a","value":"model.safetensors: 100%"}},"edbe2ea0b351435895b988305d40de89":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eee3b475e88d4dbe9061964173a0031f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eefe5a56e14340aabf09bf83e1a9a16e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f826b8cca056498a97304065a3da9291":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f96d9c8bc3eb4706ac69ae3b68bb7a3b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fed8e6d10edf41b0a6c86764b502204f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f96d9c8bc3eb4706ac69ae3b68bb7a3b","placeholder":"​","style":"IPY_MODEL_a4cb44dc8c47446c8dfb2e3aeaba6c6c","value":" 49.0/49.0 [00:00&lt;00:00, 853B/s]"}}}}},"nbformat":4,"nbformat_minor":0}
